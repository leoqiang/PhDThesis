\chapter{The cross-platform compendium creation methodology and COMMAND 
system}\label{ch:command}
\chaptermark{Command}


\instructionsintroduction


\section{Introduction}

Microarrays are one of the main technologies for large-scale transcriptional gene expression profiling. To promote data sharing, scientific journals generally require the deposit of these high-throughput experiments in public databases, such as Gene Expression Omnibus (GEO) \cite{Barrett2011} or ArrayExpress \cite{Parkinson2009}, upon publication. These databases are an extremely rich source of information, containing freely accessible data for thousands of experiments and a multitude of different organisms, and in theory provide an opportunity to analyse gene expression of a particular species at a global level. Furthermore, they hold the potential to expand the scope of any smaller scale study: mining the information contained in such databases offers molecular biologists the possibility to view their own dedicated experiments and analysis in light of what is already available. So far however, this wealth of public information remains largely untapped because these databases do not allow for a direct and integrated exploration of their data. The opportunity of combining all public experiments for a single organism has not been fully explored due to practical issues that can ultimately be attributed to the large heterogeneity inherent to microarray data. Data sets originate from different experimenters or labs and microarrays do not constitute a uniform technology. Multiple microarray platforms exist and are manufactured in different ways.  Even for similar platforms, protocols for sample preparation, labeling, hybridization and scanning can vary greatly \cite{Kerr2001,Zakharkin2005}. Although community standards specifying the mandatory minimal experimental information accompanying each dataset (e.g., MIAME \cite{Brazma2001}) have been long established, the lack of the requirements \cite{Brazma2009} imposed regarding the format of the platform descriptions and the expression measurements, as well as the degree of preprocessing done on these values further complicates the matter of experiment integration from a practical point of view.


Despite such difficulties, several initiatives exist to actively build expression compendia from public resources. Most existing compendia can roughly be divided in two groups \cite{Fierro2008}: those that directly integrate single-platform experiments, and those that indirectly integrate cross-platform experiments.  Combining data from a single platform makes the in-between experiment normalization and probe mapping relatively straightforward, so that the quantitative measures of gene expression can be analysed directly across experiments. Most single-platform compendia databases, such as for instance M$^{\textrm{3D}}$ \cite{Faith2008}, or the commercial Genevestigator \cite{Hruz2008}, focus on Affymetrix, one of the more robust and reproducible platforms \cite{Bammler2005, Irizarry2005}. Combining data from different platforms, even to the extent of combining data from single- and dual-channel microarrays, is generally done by indirect meta-analysis as opposed to directly integrating the actual expression values: one first applies the desired analysis procedure (e.g., identifying differentially expressed genes, clustering gene expression profiles, etc.) on each single data set within the compendium separately, and subsequently combines the derived results. These compendia are often topic-specific, collecting all publicly available experimental information related to a subject matter of interest.  ITTACA \cite{Elfilali2006} and ONCOMINE \cite{Rhodes2007}, for instance, focus on cancer in human; Gene Aging Nexus \cite{Pan2007} on aging in several species. There are exceptions though, such as the large ATLAS \cite{Kapushesky2010} initiative from ArrayExpress.


The compendia created by directly integrating data across experiments have the advantage of retaining actual expression values, which broadens the scope of potential analysis procedures compared to indirect meta-analysis. Most of such compendia center on eukaryotic organisms, for which considerable amounts of data are available. Relying on only one platform can still lead to sizable compendia with a broad scope in condition content, such as the human compendium constructed based on the Affymetrix U133A platform with over 5000 samples \cite{Lukk2010}.  However, for most species (e.g., \textit{Zea mays}), no single platform has such a dominant role. Even for well studied model organisms, such as {\it E. coli}, much less data is available on individual platform and a significant portion of the data is missed out when considering only one platform.


 To have the advantage of direct integration, while not being limited to a single platform, and to facilitate compendium creation from public data, we have devised methodology that directly combines expression data across platforms and experiments. The methodology has enabled us to create comprehensive compendia incorporating most high quality public data covering a broad range of experimental conditions as well as extensive types of biological samples across the boundary of experiment and platform. Although powerful the methodology is, to create a compendium is still a complex and time-consuming task. To facilitate compendium creation and the continuous curation and expansion of the existing ones, a software system COMMAND (COMpendium MANagement Desktop) has been developed. The system guides user through every step of the compendium creation process with intuitive web interfaces. Although the complete options for each step are provided for advanced users, the merit of the system is that it enables quick sizable compendium creation by general user through simple automated `one-click' executions of various compendium creation steps, alleviating the complexity of this process. This has allowed us to create multiple compendia that can be utilized not only to study a single organism but also to compare across species to study evolution and conservation.








\section{Methods}


\subsection{Cross-platform expression compendium}

The final goal of creating a cross-platform compendium is to generate a single data matrix that combines experimental results coming from multiple microarray platforms performed under variant technical protocols.  All genes measured by those microarrays should be represented, as well as all the experimental conditions that are under consideration.  Differences due to technical features (platform, protocol, etc) should be removed to make the data comparable across experiments and platforms. The rows of the compendium correspond to the known genes of the organism in question constructed based on the corresponding RefSeq file at NCBI \cite{Pruitt2007}. Uniquely, each column of it is a `\textit{contrast}', which does not represent a single biological sample but the differences between a pair of samples, one as test and the other reference. Consequently, the expression values themselves are calculated as expression log-ratios representing the gene expression changes induced by the differences between this pair of samples. Converting absolute measurements of expression into expression changes is the principal means for rendering expression values comparable across platforms and experiments. Relative expression calculated intra-experiment/platform (i.e. between two conditions measured in the same microarray experiment using one platform) negates much of the platform and experiment specific variations that make it impossible to reliably compare the absolute quantities reported in different experiments \cite{Shi2006}.



\subsubsection{The database schema for expression compendium}\label{sec:command-db-schema}

\begin{figure}
  \centering
  \includegraphics[width=1\textwidth]{DB_schema.pdf}
  \caption[The database schema for expression compendium]{ \textbf{The
      database schema for expression compendium}.
  }
  \label{fig:comp-db-schema}
\end{figure}

As the database representation directly reflects the different types of information and their relations that need to be extracted from public data to construct compendium, we give a brief explanation about it here. The compendium is stored in a MySQL database. An abstract schematic representation of various data content and their relation is shown in Figure \ref{fig:comp-db-schema}. The schema is separated into two parts. The entities in the upper part form the data of a compendium, and are publicly accessible. Those in the lower part are the data collected to build compendium, hence are only accessible internally.

Here we briefly explain each entity in the schema, which are relates to how conceptually the experimental data is organized. A gene expression \textit{experiment} refers to a set of arrays that are designed to obtain expression data in order to answer a specific biological question. An \textit{array} corresponds to one microarray in an experiment on which the biological sample(s) are hybridized to obtain gene expression measurements. A \textit{hybridization} refers to individual sample that is labeled and hybridized on an array. For an array using single-channel platform like Affymetrix, there is only one hybridization per array.  Whereas there are two hybridizations, one per channel, for the array using dual-channel platform. A \textit{platform} denotes a specific microarray chip design and the corresponding probe annotation information, such as, the probe sequence, the physical layout, the target gene, etc. The \textit{raw data} are the original measurements that come from each probe in a microarray chip.  They are also linked to the corresponding hybridization that reflects their sample origin. Three types of data are accepted, the raw expression value, the background intensity, and the background corrected expression value. A \textit{sample} refers to each individual biological sample that is used in an experiment.  Biological duplicates are treated as different samples. Note that a hybridization is an instance of a sample that is hybridized to a specific microarray chip.  When the same biological sample are hybridized onto several microarrays, they are referred as one sample but different hybridizations in the compendium. As explained in previous section, a \textit{contrast} specifies a comparison between a reference sample and a test sample, and the \textit{norm data} are gene expression log-ratios. We use a relaxed definition for \textit{gene}, which includes not only the protein coding sequences, but also other sequences that are expressed and functioning, given that their expression level can be quantified. There are also three supplementary entities, the article, the contrast annotation, and the gene annotation. The \textit{article} stores the publications related to an experiment. The \textit{contrast annotation} contains, for each contrast, the sample characteristics and the experimental factors that differ between the pair of samples. The \textit{gene annotation} stores the functional annotation of each gene collected from external sources, including metabolic pathways, Gene Ontology annotation, transcriptional regulation, and transcription units. The contrast and gene annotations are stored to facilitate the query and the biological interpretation of the expression data in compendium.

\subsection{The compendium creation methodology}
\label{sec:colombos-comp-method}

\begin{figure}
  \centering
  \includegraphics[width=1\textwidth]{COMMAND_Workflow.pdf}
  \caption[Cross-platform expression compendium creation methodology]{
    \textbf{Cross-platform expression compendium creation methodology}.  From
     left to right, three box represent three main steps of the cross-platform
     expression compendium creation methodology.  The corresponding
     functionalities are specified in the rectangles inside of each box.
     Below, the boxes shown the snapshots of the corresponding COMMAND
     interfaces.}
  \label{fig:command-workflow}
\end{figure}

The cross-platform compendium creation methodology is composed of three major steps (Figure \ref{fig:command-workflow}), namely, data collection, experiment annotation, and data homogenization. First, in the data collection step, the expression data and the accompanying (experiment and platform) information are retrieved from the online repositories, overcoming the issue of the prevalent data representation discrepancies. Next in the annotation step, the contrasts are constructed by assigning a pair of samples, one as reference and the other as test.  The experimental factor and the characteristic differences between two samples are curated manually for each contrast and specified using a set of controlled in-house vocabularies. At last, in the homogenization step, the raw expression data are normalized and log-ratios are calculated based on the sample contrast designation to create the compendium data matrix. In the following sections, we will explain the individual step in detail.


\subsubsection{Step 1: Data collection}

To create a compendium, the gene expression data need to be collected from online public repositories, GEO and ArrayExpress. Although community guidelines such as MIAME \cite{Brazma2001} exists, the failure to provide a standard format to represent information of an expression experiment causes widespread representation discrepancies among the data deposited in those online repositories. The prevailing discrepancies coupled with the colossal amount of the available data makes systematical expression data retrieval a daunting task. To make this task viable, we develop a semi-automatic workflow designed to tackle various data discrepancies and automate various processing procedures to handle the huge volume of the data. 

Here, we first explain the different sets of data to be retrieved, and then, discuss the data retrieval issues and the corresponding handling strategies for GEO and ArrayExpress separately, as each of them utilizes its own data reporting format and has its own specific data problems.


\paragraph{Data retrieval goal}\label{sec:command-data-goal}

There are three sets of information to be extracted, the experiment metadata, the platform specification, and the raw expression values. The experiment metadata that describes the sample attributes and experimental factors driving the observed gene expression changes are essential for the data exploration and interpretation. Additionally, the metadata also specifies the relations between the samples and the microarray platform used, and the corresponding data tables or files containing the obtained values. As the expression value is measured by individual probe of a microarray, the measurements reported are for each probe instead of gene. The platform specification contains the probe-to-gene correspondence that is essential for converting probe measurements into gene expression values. The raw expression values are the foundation of the compendium.  The quality of the compendium depends directly on the raw data's quality. The original probe intensities without background correction are preferred raw data, as the background correction has been shown to increase data variance especially at the low intensity range \cite{Irizarry2006, Ritchie2007}, and the processed data contains artificially introduced inconsistency due to the different normalization algorithms applied. An in-house normalization pipeline is utilized on the collected raw data to achieve better consistency. Except for extracting desirable data, equally important is to retain proper value-to-probe mapping and value-to-hybridization association (see Figure \ref{fig:comp-db-schema}). The former is required to convert the raw probe intensity into the gene expression value. The later links the expression value with the corresponding biological sample, hence the associated sample attributes and experimental factors.


\paragraph{Step 1.1: Experiments retrieval, filtering, and data downloading}

Although started with gene expression data, GEO and ArrayExpress are extended through the years to include other functional genomics data, such as, ChIP-chip experiments. Furthermore, certain types of expression data, e.g., those of the cross-species gene expression comparison studies, are not interesting for a compendium specific for one species. It is then preferable to start with a clean list of experiments.

To maximally automate this process, the programmable access interfaces, Entrez Programming Utilities (E-Utils) of GEO and REST query API of ArrayExpress, are utilized to retrieve the basic metadata of all experiments belong to a given species. Note that GEO, as the earliest of two, is served as our primary data source, whereas ArrayExpress supplements GEO by providing extra data not available in GEO. Upon obtaining the list, the existing experiments are removed.  For the experiments common in both repositories, only the corresponding GEO record is kept. The remaining experiments is sifted through filters based on a set of in-house collected key phrases to remove undesirable experiments, e.g., ChIP-chip data, cross-species comparison data, etc. The filtering is rather conservative to avoid excluding experiments by mistake. The remaining experiments are then reported as new ones.  A human inspection on this list of new experiments is recommended, although not mandatory. When definite, the corresponding data file(s) are then downloaded from online repositories for data collection.        


\paragraph{Step 1.2: GEO data extraction} Here we first explain how the expression data are organized in GEO and then the strategy to extract data desirable for compendium creation. 

\subparagraph{Expression data in GEO} There are two sets of data for an experiment in GEO, the \textit{GEO Series} (GSE) containing the original submitted data and the \textit{GEO DataSet} (GDS) containing curated normalized data. To create compendium, we collect original data stored in GSE. The data of an experiment is stored in a standard simple line-based plain text format, called SOFT (Simple Omnibus Format in Text).  There exists one SOFT file per GSE (experiment) stored in compressed (gzip) form. The file contains one or multiple instances of three types of records: Series, Sample, and Platform (see Table \ref{tab:GEO-records}). The \textit{Series} record contains general information about various aspect of an experiment described in free text.  The references to the corresponding Sample and Platform records are also provided. The \textit{Sample} records corresponds to one microarray chip used in an experiment. It is composed of two sections.  The first one provides free text descriptions about biological samples hybridized to the microarray, including information about sample attributes and experimental factors crucial for compendium data exploration and interpretation. The referral to the related Platform record is also provided here. In the second section, the expression measurements are reported for each probe (or probeset) of the microarray in a tabular form. Often the values reported here are processed data instead of the desirable original probe measurements. The \textit{Platform} record describes the microarray chip used to measure gene expression, including a free text description and its probe annotations in a tab-delimited table. Specific about the required content in various records and their sections, the SOFT format does not impose strict requirements about the representation of the data. Consequently, the expression data reporting format and the probe annotations varies among experiments and platforms. For many experiments, raw data files are also available as supplementaries providing an alternative source to obtain desirable gene expression measurements. In original vender specific formats, the raw data files differ among software and/or equipment.  Often allowing to be used alone, the file includes probe specification for each record in addition to the expression value(s). Note that as the submission of the raw data files is optional, many experiments do not have supplementary. Every record in GEO has a unique access id. Its `Series', `Sample', and `Platform' records correspond to the experiment, array, and platform object in our compendium respectively (see Table \ref{tab:GEO-records} and Figure \ref{fig:comp-db-schema}). 

\begin{table}
\begin{centering}
\begin{small}
  \caption{GEO Records en Files and their corresponding content}
  \label{tab:GEO-records}
  \begin{tabular}{@{\hspace{1mm}}>{\raggedright}p{1.8cm} >{\raggedright}p{2cm} 
      >{\raggedright}p{7.2cm}@{}}

    \toprule

      & \textbf{Compendium} &  \tabularnewline
     \textbf{Record} & \textbf{entity} & \textbf{Information contained} 
     \tabularnewline

    \midrule 

    Series & Experiment & experiment metadata 
    \tabularnewline\hline

    Sample & Array & sample characteristics, hybridization(S) 
    (channel)
    information, and data table (expression measurements) \tabularnewline\hline

    Platform & Platform & platform metadata and probe 
    annotations \tabularnewline\hline

    Supplementary files & Array or Hybridization & raw expression values 
    accompanied with probe annotations in vendor specific format \tabularnewline

    \bottomrule
  \end{tabular} 
\end{small}
\end{centering}
\end{table}

By default, to obtain expression data from an GEO experiment, the corresponding SOFT file is analyzed.  The contents of different records contained in the SOFT file are separated first, and then handled by the specific parsers to extract data. Only when the desirable data cannot be obtained, the corresponding supplementary files are processed.


\subparagraph{Step 1.2.1: GEO experiment metadata extraction} The experiment metadata includes the basic experiment information in a GEO  `Series' record (e.g., name, description, publication, etc) and the sample  attributes and experimental factors in the corresponding `Sample' records. As mentioned before, instead of using controlled vocabulary, this set of information is described in free text, which lacks consistency as often one concept can be described in multiple ways. Additionally, often the metadata available is incomplete, and the missing information needs to be manually extracted from the related articles. The inconsistency and incompleteness of the experiment metadata renders a computerized data extraction impossible.  Instead, a manual curation (in the annotation step) is required to analyze and standardize the information into a computable format to facilitate data exploration. Consequently, this information is extracted as is without modification. And the direct correspondence between those GEO records and the compendium objects makes the extraction straightforward.   


\subparagraph{Step 1.2.2:  GEO platform data extraction} The GEO `Platform' record contains both the basic platform description and the probe annotations. The former, including name, description, manufacturer, etc, are extracted as it. Although the probe annotation is always presented as a table, each platform nevertheless has its own format that varies in the amount of content (number of columns), the column naming convention, and the content of individual column. Ideally, the probe sequences should be provided so that a homology search against the latest gene sequences using BLAST \cite{Altschul1997} can identify the up-to-date gene target for each probe. However, although required by MIAME\cite{Brazma2001} standard, this information is missing for the majority of the platforms. Alternatively, the target gene can be identified by other information, namely, locus tags, alternative gene tags, or common gene names. But the lack of a consistent format makes it hard to identify the corresponding column providing those information. Even worse, sometimes, the relevant information is embedded in a column in which multiple contents are specified in a complicated structure. Additionally, the inconsistency also exists in the content of one data column, in which different types of information are found. For example, locus tag, gene name, or other tags are often used alternatively in one data column titled 'ORF'. 

The key for platform data parsing is the ability to identify data columns containing useful information and provide proper methods to extract a standardized set of information for every platform. Due to the format heterogeneity of platform data, a user-guided semi-automatic procedure that couples manual data column identification with an automated data extraction is developed. First, the probe annotation table is checked manually to identify data columns containing desirable information and specify the method to extract data. Whiling collecting platform data for parsing, an in-house dictionary based content identification method is applied to analyze the column names of the annotation table and automatically mark out the candidate columns. Helping to alleviate the manual inspection task, it cannot, however, replace the human inspection, as it cannot handle information embedded in a complex format and the column name based identification is not 100\% reliable. Even with data columns identified, the existence of different formats that can be used to specify one type of content complicates data parsing. To automate the content parsing task, a plug-in based system with great flexibility and extensibility is developed. The existing nine plug-ins together with the direct `copy' option are capable of handling every platform included in the compendia already developed, and the system can be easily extended with new plug-ins. Next, after manually identifying interesting data columns and assigning proper function to parse them, the data are extracted utilizing our plug-in system, and the target genes are automatically identified. When identifying target gene without sequence information, the aforementioned content inconsistency is handled by an integrated search over multiple information sources in strict order of preference, locus tag, alternative gene tag, and then common gene names, based on its reliability.


\subparagraph{Step 1.2.3: GEO raw value data extraction}  As mentioned previously, the expression value extraction has two goals, to extract desirable raw expression data and to retain proper value-to-probe mapping and value-to-hybridization association. In GEO, the expression data can be extracted from two different sources, the `Sample' record and the supplementary file. The former is the primary data source, as it is readily available in SOFT file.  The supplementary file is checked only when the desirable data cannot be extracted from `Sample' records. The expression value extraction for the single-channel array is easier than that for the dual-channel array, as the value-to-hybridization association is straightforward, given that only one hybridization exists. To appreciate the full complexity of the expression value extraction task and to demonstrate the full capacity of our strategy, we assume that the data to be handled are generated by a dual-channel microarray, whereas the single-channel microarray data can be handled by the same strategy leaving out the value-to-hybridization association extraction part.





\subparagraph{\textit{Step 1.2.3.1: Parse GEO Sample record}}
Each GEO `Sample' record has a data section that reports expression value in a tabular form. Although only processed results are required to be reported here, often they are accompanied by the corresponding raw intensities. As part of the SOFT format, the proper value-to-probe mapping is guaranteed through using the same probe id consistently across the corresponding `Sample' and `Platform' records. This simplifies the data extraction task.  Hence it serves as our primary source for raw data extraction.

To successfully extract raw data, two tasks remain, to properly identify the desirable raw expression value and to obtain correct value-to-hybridization associations. The first task is hindered by several issues.  First is the lack of standard data reporting format in `Sample' record, instead the data table of the corresponding raw data file is often used. The existence of a large number of raw data formats creates widespread data representation discrepancies. Although it is mandatory to provide the data table header descriptions explaining the content of each data column, depicted in free text, it cannot be readily analyzed by computer. Second, measuring two samples in one chip, it is crucial to identify the channel association for each data to be extracted.  However this information can only be derived indirectly through analyzing the column name. At last, sometimes, only the background corrected values instead of the original ones are reported.  As the correction results in an increased variance for lower, less reliable intensity levels \cite{Ritchie2007}, these data are not desirable. When the corresponding background value is available, the uncorrected intensity can be reconstructed from the data.  However, this requires that the system is capable of automatically identifying and applying certain data conversions. Note that since raw intensities are reported as numbers, the data extraction is straightforward after the corresponding columns are identified. For the task to obtain value-to-hybridization associations, the channel related information extracted from column name needs to be paired with the hybridization information specified in the `Sample' record. Although `ch1' and `ch2' are used in the `Sample' record, various types of information are specified in the data column names.
 
Given the above issues, we developed a semi-automatic raw expression value extraction system. The fully automatic extraction is triggered under a strict condition, in which the raw expression value without background correction and the corresponding background values are extracted, the channel information of each extracted data is properly identified, and the value-to-hybridization associations are corrected derived. Such a strict condition guarantees that correct data are extracted by the system. The key of the automatic data extraction lies in the ability to programmatically identify the data content type and the channel information avoiding manual inspection. Our solution is a rule based column name analysis subsystem that analyzes the name of data column using pattern matching to identify required information. Targeting the desirable data, the system focuses on identifying three types of information, the raw expression intensity value ($I$), the background value ($BG$), and the background corrected expression value ($I_{BG}$). For each data type, the related columns are collected from large number of Sample records.  The patterns of the column names are then manually analyzed, and the rules to recognize the data type and the channel related keywords are derived and used in the system. Next, the extracted channel keywords are checked against the hybridization specification to obtain value-to-hybridization association. Three types of keywords are handled, the direct channel specification (`ch1' and `ch2'), the dye color specification (`cy3' for green and `cy5' for red), and the dye frequency specification (`532' for green and `635' for red). The first type can be directly mapped to a hybridization, whereas for the last two types of keywords, the hybridization labeling information is required. After all columns are analyzed, the system checks those identified and automatically discover and set the data conversion function when necessary. At last, the aforementioned conditions are checked against the information identified by the system. When succeed, the raw expression data are then automatically parsed. Otherwise, the corresponding error is reported to highlight the issues for manual inspection.      

\subparagraph{\textit{Step 1.2.3.2: Parse GEO supplementary file}}

When the `Sample' record does not provide desirable raw values, it is still possible to extract them from the raw data file that supplements the `Sample' record. Note that designed to be self-contained, the raw data file includes not only the expression value data, but also the probe annotations and various level of meta information. Except for the same issues encountered when parsing `Sample' record, there are other issues when parsing raw data files. First, as mentioned before, there exist a large number of different formats for raw data file, which varies on the mount of the meta information available, the probe annotations used, and how the raw values are reported. Hence, each format needs a specific parser to handle it, and to automate the process, the system should be able to automatically recognize the file format. Second, as the raw data file is not part of the SOFT format, the probe annotation specified in the file do not use the same GEO probe id specified in the corresponding `Platform' record. Whereas to improve the consistency across the experiments using the same platform, GEO probe id is preferred. Consequently, the probe information in a raw data file needs to be mapped to GEO probe id. It should be noted that as the reporting format used by raw data file is not platform specific, hence the parser should be flexible to handle different platforms. At last, the relationship between a `Sample' record and its corresponding raw data file is not always reported, hence the system should have the capability to identify it. 

To guarantee the extraction of correct data with proper probe and hybridization associations and to automate the parsing process, several format specific parsers are developed to handle most popular raw data file formats. Currently, following formats are supported, GenePix Results format (GPR), and Perkin-Elmer ScanArray format.   However the NimbleGen Pair format and Imagene format is not supported, as manual intervention is required to handle them\footnote{The results of a   dual-channel platform reported in these formats are split into two data   files, one per channel.  It is often impossible to programmatically derive   the value(file)-to-hybridization association.}. Each data format is thoroughly studied to develop the corresponding parser. Similar to `Sample' record data parsing, the raw value data columns and the corresponding value-to-hybridization associations are identified from column names. The focus of a parser is to provide format specific method to obtain accurate value-to-probe mapping, which should be flexible to handle different platforms. A greater accuracy is achieved by utilizing the well structured and accurate parsed probe data of the corresponding platforms to search against the standardized probe information provided in a raw data file. A match could be identified in multiple manners applied in the order that favors the most accurate one applicable, providing flexibility to handle the amount of information varies across platforms. Consequently, to parse a supplementary raw data file requires that the corresponding `Platform' record has been parsed obtaining as much information as possible for probe matching. 

The supplementary file is parsed by a semi-automatic three-step procedure. First, the raw data file to GEO `Sample' record association is either obtained from the `Sample' meta data or derived from the existence of the GSM access id in the name of the corresponding file. Next, the format of the raw data file is identified by either the file extension or its meta information signature. When identifiable, the corresponding dedicated parser is then applied to automatically extract raw values and assign them to the proper probes and hybridizations. For the file of unknown format, a manual data parsing is provided as an alternative. The new formats encountered are tracked, and a dedicated parser can be added when necessary. To avoid the overcomplexity, the automated system only handles the case where there is only one raw data file per `Sample', and supports only the ASCII file and the excel binary file (csv or xls).








\paragraph{Step 1.3: ArrayExpress data collection}

\subparagraph{Expression data in ArrayExpress}

In ArrayExpress, the experiment data is stored in the MicroArray Gene Expression Tabular (MAGE-TAB) format. In this format, 4 types of file capturing different set of information are used, namely, the Investigation Description Format (IDF) file, the Sample and Data Relationship Format (SDRF) file, the Array Design Format (ADF) file, and the raw and processed data files (normally packed into one zip file) (see Table \ref{tab:AP-files}). Similar to the `Series' record in GEO SOFT format, the \textit{IDF} file provide an overview for an experiment. Although the MGED ontology terms are used, they specify only the type of the information.  The content, however, is still described in free text. The \textit{SDRF} file describes the sample characteristics and the relationship between samples, arrays, and data files. The content is presented in a tabular form, in which each record (row) describes information about one biological sample hybridized to a specific channel of a specific microarray chip, hence corresponds to a hybridization in our compendium (Figure \ref{fig:comp-db-schema}). The references to the corresponding platform are provided for each record. One caveat about the SDRF file is that its format is not strictly defined, and can vary among experiments. Next, the \textit{ADF} file provides information for a microarray platform, including both the descriptive information and the probe annotations. At last, the expression values are reported in the raw and/or normalized data files. In ArrayExpress, the unique access id is given only to experiment and platform, which directly corresponds to the experiment and platform object in our compendium (Figure \ref{fig:comp-db-schema}). However, there is no explicit record in ArrayExpress that corresponds to the array object.  Additionally, although each record in SDRF file corresponds to the hybridization object, without a unique reference, this information is only indirectly accessible though the corresponding ArrayExpress experiment.


\begin{table}
\begin{centering}
\begin{small}
  \caption{ArrayExpress data file format and corresponding content}
  \label{tab:AP-files}
  \begin{tabular}{@{\hspace{1mm}}>{\raggedright}p{3cm} l >{\raggedright}p{5.8cm}@{}}

    \toprule 
                  & \textbf{Compendium} &  \tabularnewline
    \textbf{File} & \textbf{entity} & \textbf{Information contained} \tabularnewline

    \midrule 

    Investigation Description Format (IDF) file & Experiment & experiment
    metadata \tabularnewline\hline

    Sample and Data Relationship Format (SDRF) file & Hybridization & sample
    metadata; links between sample, array (indirect), and data file
    \tabularnewline\hline

    raw data file & Raw data & expression values \tabularnewline\hline

    Array Design Format (ADF) file & Platform & platform metadata and probe
    annotation \tabularnewline 

    \bottomrule
  \end{tabular} 
\end{small}
\end{centering}
\end{table}

For an ArrayExpress experiment, the corresponding IDF and SDRF files together with the raw data file are downloaded. The first two contain experiment metadata, and the last one the expression values.  Additionally, for each new platform, the corresponding ADF file is downloaded and handled separately.

\subparagraph{Step 1.3.1: ArrayExpress experiment metadata extraction}

For an ArrayExpress experiment, the experiment information is described in the IDF file, whereas the sample attributes and experimental factors are described in the SDRF file. The metadata extraction from the IDF and SDRF files are not so simple. The first issue is the data content inconsistency in SDRF file, as it has no strictly defined format. The data can be submitted into ArrayExpress using different methods, e.g. MIAMExpress, MAGE-TAB, etc, generating compatible but not exact the same set of information. For example, `Hybridization'\footnote{Noted that this `Hybridization' is   similar to the GEO `Sample' record, which refers to one microarray chip   used in an experiment and corresponds to the compendium array object.   This is different from the compendium hybridization object that   corresponds to one channel of an array.}, which is mandatory in MIAMExpress, is replaced by `Assay' in MAGE-TAB, which is not obligatory and often missing. Second, there lacks of an object corresponds to the array object in our compendium, which uniquely refers to one microarray chip and bundles the corresponding channels together. For a dual-channel microarray, it is crucial to pair two channels together, as their raw data are often jointly normalized. Recall that each SDRF record (row) corresponds to one channel of an array, this relation between channels then needs to be derived indirectly from the data content. Due to the data inconsistency, multiple data sources (columns), e.g. `Array Data File', `Scan Name' or `Hybridization Name', are utilized in this process, depending on their availability. When this relation is identified, the corresponding SDRF records are paired and assigned as different hybridization(s) of an compendium array object. When the system fails to identify this relation, a manual inspection is required.

\subparagraph{Step 1.3.2: ArrayExpress platform data extraction}

For each platform in ArrayExpress, its data are specified in the ADF file, which needs to be downloaded separately. Similar to the GEO `Platform' record, there are two sections in an ADF file, the platform descriptions and the probe annotation table. However, the platform probe annotation table in ADF file follows a standardized format with a fixed set of column names and rather simple data content without complex structures. An automated data extraction method targeting the preselected data columns that provides desirable probe data is used to parse ADF file, avoiding the issue of the data column content type identification that plagues the GEO platform parsing. Although the content inconsistency still exists, it can be handled using the integrated search strategy developed for GEO platform data parsing.

   \subparagraph{Step 1.3.3: ArrayExpress raw data extraction}

In ArrayExpress, the expression values are reported in the raw data files and normalized data file. The raw data file is our focus as they provide the data desirable for creating compendium. There exists two kinds of raw data files, the vender specific data files, e.g. Affymetrix CEL file, Genepix GPR file, etc, and the processed raw data files which is normally a reformatted one. The former are used mainly for the experiments based on Affymetrix platforms, and occasionally, for the experiments using GPR files. The Affymetrix specific data files (CEL file) are handled by a dedicated procedure that will be explain later. The experiments using original GPR files are very rare.  There are only two found while creating all four existing compendia.  As the probe information specified in GPR file could not be mapped to the corresponding ArrayExpress platform ADF specification without lose of information, they are simply skipped for now. The majority of the experiments, including most of those originally using GPR files, reports expression values using the processed raw data files, which is a tab delimited text file contains only an adapted data table without meta information. Two kinds of changes are applied on the original data table.  First, the platform related part of the table, containing probe annotations, is replaced with the corresponding standard ArrayExpress ADF platform annotation. Consequently, it is straightforward to correctly obtain value-to-probe mappings between the raw data file and the ADF file. Second, the column names of the expression data section of the table are augmented with different types of information.  The most common ones include but not limited to the original file format, the corresponding `Hybridization Name' specified in SDRF file, or the original data file name. In a majority of cases, this feature allows a separation of the platform related data from the expression value related data, and the recovery of the original column names.
 
The raw data extraction is processed only when it is possible to link the extracted expression values to the corresponding biological samples. Note that the SDRF file parsing has successfully reconstructed channels relations and created the corresponding compendium array object. However there still exits two missing links to connect raw data to sample. The first link is to identify the corresponding raw data file for each array object.  Generally, the `Array Data File' column in SDRF record specifies this information. When that is missing, we notice that the extra information added into data columns sometimes provides clues, for example, when it matches the hybridization name of a SDRF record. In those cases, the system is capable of automatically identifying the data file correspondence. Otherwise, a manual inspection is required when this correspondence cannot be identified. The second link is to obtain the value-to-hybridization associations for the data generated on dual-channel microarray platform. As the original raw data column names can be recovered, the same rule based column name analysis subsystem developed for GEO data parsing is utilized to extract the dye color information from column names, which is then checked against the `Label' of a hybridization obtained from SDRF record to recover the required associations.

Hence the ArrayExpress processed raw data file are parsed in five steps. First, the data file to compendium array object association is checked or identified.  Next, the data file headings are analyzed to separate the probe annotation columns from the raw data columns.  Then the raw data column names are analyzed to recover value-to-hybridization association.  At last, the same set of data quality conditions applied in GEO `Sample' data parsing are checked.  When met, a automatic data retrieval is triggered that extracts each channels raw values for the corresponding hybridization and maps each value to the correct probe. Similarly, when it fails, the corresponding error is reported to highlight the issues for manual inspection.
















\paragraph{Step 1.4: Affymetrix platform data and expression value extraction}\label{sec:command-affy}


The single-channel Affymetrix microarray is, by far, the most widely used platform due to the high consistency among the results obtained across labs compared to other platforms\cite{Irizarry2005}. The use of the proprietary file formats to specify platform information (Chip Description File, CDF) and to report expression values (CEL file) simplifies the data retrieval. 

In an Affymetrix chip, each gene is targeted by a group of short oligonucleotide probe pairs, collectively called a \textit{probeset}. Each probe pair is composed of a Perfect Match (PM) probe and a MisMatch (MM) probe, in which the PM probe measures the target gene expression level and MM probe measures the background signal. This specific design of Affymetrix microarray chip creates two possible platform specifications, one for probesets and the other for probes. The former is reported in GEO and ArrayExpress, whereas the later can be retrieved from the corresponding CDF file. Consequently, two levels of expression value exist, the raw intensity of each probe, and the summarized intensity of each probeset. Many algorithms \cite{Irizarry2003, Li2001, Hubbell2002} have been developed to normalize Affymetrix data and compute the summarized expression value for each probeset. To avoid the inconsistency introduced artificially by different algorithms, we opt to obtain the raw probe intensities from the CEL file, then processed using our in-house homogenization pipeline based on RMA algorithm. The background values measured by MM probes are ignored as it has been shown that though the background correction improves accuracy, it greatly sacrifices the precision\cite{Irizarry2006}.
 
Occasionally, an experiment using Affymetrix platform reports only the summarized expression values in the online repository. This requires that our system is able to handle both raw intensities and the summarized values, and in turn, requires retaining both the probeset and probe annotations, and the relations between them. GEO and ArrayExpress onlystore the probeset annotations of an Affymetrix platform. To keep it consistent, the corresponding compendium platform record contains the same annotations. Additionally, for each Affymetrix platform, an extra platform record, which is always associated with the original one, is created artificially in our compendium. This kind of platforms are called `virtual platform' as there exists no directly correspondence in the online repositories. Next, by incorporating the proprietary Affymetrix Fusion SDK into our system, the probe specifications are extracted from the corresponding CDF file downloaded from Affymetrix website and stored with the virtual platform. 

With platform specification ready, the raw intensities can be easily extracted from CEL file. As a single-channel platform, each CEL file contains the expression values for only one hybridization (and one sample). For an experiment whose CEL files are available and the file-to-array correspondence has been successfully derived (using aforementioned repository specific methods), the raw intensities can then be automatically extracted from CEL file using Fusion SDK. As both CEL and CDF files are Affymetrix proprietary formats, the same probe references are used consistently. At last, the corresponding compendium array object is linked to the virtual platform instead of the original one to reflect the fact that the probe level raw intensities are retrieved as raw data instead of the summarized values. If the CEL file is missing, the summarized values are used. 






\subsubsection{Step 2: Annotation}

After obtaining experiment data from online repositories, the contrasts that will be represented in the compendium are defined.  The experimental factors and the sample attributes are carefully studied to construct a rigid annotation for each contrast.  

\paragraph{Contrast designation}
Based on their biological role in an experimental survey, hybridizations are labeled as `reference' or `test' on a per experiment-and-platform combination basis and matched to produce a set of contrasts.

For dual channel experiments, usually one of two hybridizations of an array serves as a reference to the other, as this inherently counters much spot associated variation in the measurements. There are exceptions however, such as when one of the hybridizations on an array does not constitute an identifiable and unique biological condition for which the transcriptome was assessed (e.g. a sample of genomic DNA or a pool of different samples that cannot be considered as biological replicates). These hybridizations are discarded and the experiment is further treated as if it was a single channel experiment. In this way we ensure that every contrast has a biologically interpretable meaning: its associated log-ratios represent expression changes in response to quantifiable stimuli altered from reference to test or the characteristic differences between samples.

For a single channel experiment, one or more hybridizations can be chosen as references for the remaining tests or each group of tests respectively. The choice depends on the nature of the experiment and the type of biological conditions that are measured. Just to give a few examples.  For a time-series experiments, the sample taken at the first time point is chosen as the reference to those taken at the other time points.  For an experiment studying the effect of mutants against the wild type, the later is chosen as the reference. A more complex example where we would choose more than one reference might be an experiment where they measure treatments with two compounds $A$ and $B$ in different concentrations. In that case we might use the treatment with lowest concentration of $A$ as a reference for all other treatments with $A$, and the treatment with lowest concentration of $B$ as a reference for all other treatments with $B$, and we additionally include one contrast comparing both references, i.e. where the lowest concentration of $A$ is considered test and the lowest concentration of $B$ is considered reference (to include an explicit comparison between $A$ and $B$). It is important to note that for an experiment using several platforms, the samples are grouped by the platform and for each group, one sample is designated as the reference for others.  This is because the probe differences between platforms render the measured gene expression intensity values incompatible.  Consequently, it unjustifies the use of a sample measured on one platform as the reference for the samples measured on the other platforms.  

\subparagraph{Multi-chip platforms}\label{sec:command-multichip-anno}
Because of the limitation of the microarray technology and the complexity of the large eukaryotic genome, it often cannot cover the complete gene set of an eukaryote in one chip. Alternatively, multiple chips of the same technology, each targeting complementary gene sets, are designed. These specially designed platforms are referred as the \textit{multiple-chip   platform}. Due to the complementarity by design, these chips are used together on the same sample(s) to achieve better gene coverage. Consequently, it is not desirable to treat these chips separately as there is littler gene overlap between them. Instead, the data generated on multiple chips originated from the same sample should be grouped together by assigning corresponding hybridizations to the target sample in the compendium. Special handling strategy has been developed to normalize this type of data (section \ref{sec:command-multichip-norm}). 


\paragraph{Sample annotation and ontology}

Using a set of formal hierarchically structured properties, we can then specify the differences for these property values between the test and reference samples as the annotation. Initially for bacteria compendia, the annotation is specified only for each contrast and categorized into four classes: genomics, growth, medium, and treatment. The genomics properties specify the difference(s) on the genomes of the two samples, including four subcategories: mapped mutations, phenotypic strains (carrying specific phenotypes with yet mapped mutations), evolutionary adaptation (genomic differences accumulated in an evolutionary experiment), and plasmid (genomic differences in plasmid DNA). The differences of chemical compounds used as the base medium and the additives are described by the medium related properties. The treatment includes general environmental properties, such as, temperature, pH, UV radiation, or oxygen level etc. The other general properties are grouped as growth properties, such as, time, growth phase, etc. Each annotation specification is defined as a duplet, including a property that is different between two samples of a contrast and a value describing to what extend this difference is. The annotation is then a vector of values, one for each characterized difference between samples. Our annotation enables a mathematical comparison and automatic organization of contrasts based on the properties that are surveyed, but it is a labor intensive manual curation process where information often needs to be retrieved from original publications, supplementary data and occasionally directly from the authors. 

The annotation properties themselves are further structured in an ontology tree employing the same classes as the well-defined Gene Ontology biological process subtree terms \cite{Gene2010}. Assigning annotation properties of seemingly distinct categories to the same ontology term reflects the fact that different properties might in fact be related to the same biological process.  For example, in \textit{E. coli} compendium, the condition ontology term response to oxygen levels includes several properties from different levels of the property hierarchy, but that are all linked to cellular processes that are dependent on oxygen availability, such as \textit{fnr} mutations (a global oxygen responsive transcriptional regulator), NO2 concentration (an electron transport decoupler), agitation of the growth medium, actual oxygen levels, etc. The ontology provides a biologically more intuitive view of the annotation, and a novel data exploration option allowing an integrated study of different aspects centered around target biological processes.

\subparagraph{Sample annotation for eukaryote} 
To handle the biological diversity of the complex plant \textit{Zea mays}, the annotation system is further expended to completely characterize, for each sample, its genotype specification (breeding line), tissue origin, and the development stage. Associated with each sample instead of contrast, these extends the scope of the  annotation to provide extra information that might be common between  the test and reference samples, yet crucial for data exploration and  interpretation. For example, the expression variation observed under a stress condition when  comparing a pair of leaf samples might differ from that between two root  samples, as different mechanisms could be employed to counterattack the same  stress in different cells of a plant.  Incorporating these annotations provides a refined characterization of  contrasts, which allows more elaborate data selections, for example, to choose  only those involving leaf tissues, as well as a better data interpretation to  include those factors into consideration. A further categorization of contrasts based on the similarity between specific  characters also becomes possible.  Four sub-compendia are created containing  only a subset of contrasts that compare between samples gathered in different  tissues, collected at variant development stages, taken from distinct breeding  lines, or with or without external perturbations respectively. Providing a sub-compendium with a confined scope could well facilitate the data  exploration for the biologist with particular research questions. More details are given in section \ref{sec:magic-anno} of chapter  \ref{ch:magic}.









\subsubsection{Step 3: Data homogenization}

The final part in the creation of a compendium is the homogenization in which the raw expression data are normalized, and the log-ratios are calculated, per experiment, between samples based on the contrast specification. Several preprocessing procedures are conducted to render expression levels comparable between different experiments and platforms. Crucial steps in this preprocessing are array-specific and depend on both the technological platform used to perform the experiment (single- or dual-channel), as well as on the reported units of expression. In general we adhere to the following principles: 
%
\begin{enumerate}
\item Raw intensities are preferred as data source over normalized data   provided by the public repository. (This is handled in the data collection   step.)
%
\item No local background or mismatch probe correction procedures are   performed to avoid an increase in variance for lower, less reliable   intensity levels \cite{Ritchie2007, Irizarry2006, Engelen2006, Li2001}.   This improves the data precision, which is crucial for our compendium, since   we do not do any `significantly differential expression' calculations, which   might be robust against the increased variance, due to the lack of necessary   biological replicates.
%
\item Non-linear normalization techniques are performed to account for global   inter-hybridization differences (e.g., a loess fit to remove dye-related   discrepancies on dual-channel arrays \cite{Yang2002}, quantile normalization   for high-density oligonucleotide experiments \cite{Bolstad2003}).
%
\item Log-ratios are calculated based on normalized intensity data.   For each dual channel array, the log-ratio is calculated between its   own hybridizations.  When multiple probes target the same gene   (technical replicates), to obtain one log-ratio per gene, the   average is taken in case of lower number of replicates, whereas   Median Polish is applied on the log-ratios of a large number of   replicate probes to obtain a robust result (summarization). \vspace{1.5mm} \\
  % 
  For single-channel arrays, the normalized intensities from technical   replicates are first summarized to produce one intensity measurement   per gene before calculating the log-ratios based on the contrast   definitions (summarization).  Median Polish is utilized when the number of   replicates per gene exceeds 4, e.g. for Affymetrix; otherwise the   average is taken.
%
\item After log-ratios are calculated, the data quality is checked on MA plot.   Often, we still observed some non-linear intensity-dependent differences in   the data, especially when Median Polish is utilized to handle the data   generated by large number of replicates.  This is then corrected through an   extra non-linear normalization (e.g., loess fit).  Similar discoveries and   handling approaches have been described in the literatures (\cite{Choe2005,     Welsh2013}).
  %
\end{enumerate}

\begin{figure}
  \centering
  \includegraphics[trim=0cm 0.5cm 0cm 9.5cm, clip=true, 
  width=1\textwidth]{COMMAND_pipelines.pdf}
  \caption[Default pipelines for compendium raw data homogenization]{
    \textbf{Default pipelines for compendium raw data homogenization}}
  \label{fig:command-pipelines}
\end{figure}

The default data preprocessing pipelines for single channel array (Affymetrix,  NimbleGen, Agilent one-color, etc) and dual-channel arrays are shown in Figure  \ref{fig:command-pipelines}. Following each step, the data quality is checked visually.  If there are issues with the result, extra steps and/or alternative methods and/or parameter settings could be utilized to correct them. Generally, the homogenization is applied per experiment.   Occasionally, some single-channel experiments conducted by the same lab share the data generated on some arrays (the same access id in GEO), especially the reference ones probing wild type. In those special cases, although originated from multiple experiments, the contrasts sharing the same reference are homogenized together to achieve better consistency. After log-ratios are calculated for each experiment individually, they are integrated into one big compendium data matrix containing the data of all the processed experiments.

\subparagraph{Multiple-chip platform data normalization}\label{sec:command-multichip-norm}
For a multiple-chip platform, we first followed the same procedure to normalize data and calculate the log-ratio for each chip separately. Then an additional step is introduced, in which the log-ratios from multiple chips of the same platform are combined per contrast. Although the chips of a multiple-chip platform are designed to be complementary, sometimes there are a few genes that are measured on more than one chip generating multiple expression values per gene. To obtain a single gene expression value per contrast, the median is taken over the multiple chips to obtain one final value for each gene.



\subsubsection{Data publishing}
After internal control, new compendia are released for public access. An existing compendium can be updated in two fashions, an incremental revision and a new release. A \textit{revision} adds extra experiments into the current version of the compendium providing more data. The new data are collected and processed independently, and directly integrated into the data of the current version of compendium after quality check. Over the time, as the understanding of the genome advances, there can be a genome revision for a species in which its gene annotations change. Recall that our compendium is a matrix whose rows correspond to genes, the change of gene annotation will change the number of rows this matrix has. In this case, a \textit{release} is initialized creating a new version of the compendium incorporating the latest gene annotations. To create a release, the new gene annotations are first imported into the system, then the platform annotations are updated to this latest gene annotations, next all experiments available in the current version are homogenized again utilizing the updated platform annotations to generate the data matrix for a new version of the compendium, and at last after quality check, the new version is released for public access.








\subsection{COMMAND web system} 

\begin{figure}
  \centering
  \includegraphics[trim=0cm 2cm 0cm 0cm, clip=true, 
  width=1\textwidth]{COMMAND_interface.pdf}
  \caption[COMMAND compendium creation en curation system]{
    \textbf{COMMAND compendium creation en curation system}}
  \label{fig:command-interface}
\end{figure}

As utilizing our methodology to create compendium is a complex multiple step process, a web system named COMMAND (COMpendium MANagement Desktop) (Figure  \ref{fig:command-interface}) has been developed integrating the methodology  with a web interface to facilitate compendium creation and maintenance. The system is composed of three components, the backend providing core functionalities for compendium creation, the MySQL database to store the data collected from online repository and the content of created compendia, and the web service providing the front-end that interacts with users.  The Apache server interfaces the communications between those components. The functionalities provided in the backend are computationally intensive and require no human interaction.  These include but not limited to compendium initialization, experiment information retrieval, raw data download, automatic data parsing, data homogenization, and publishing. The peripheral functionalities are implemented directly in the web system, such as, user control and management, new compendium initialization, etc.  So are those functionalities that require human interactions, such as, various manual inspection functions, data annotation, etc.  Additionally, data dependencies between various steps of the methodology are controlled through web interface. 

\paragraph{Availability} 
The compendium creation system can be installed and configured to run on any LAMP server with Matlab runtime environment installed.  Practically, a dedicated MySQL server can improves the performance of the system. The code is not publicly available but can be provided upon request.
  




\section{Results and Discussion}
 
Here, we present a methodology that is capable to integrate high quality expression data of different experiment and platform origin in a consistent manner, and creates a species-wise expression compendium accompanied with high quality manually curated annotations that facilitates both the global scale analysis and the targeted data exploration. Although the abundant data is publicly available on repositories such as GEO and ArrayExpress, the individual experiment still remains isolated and disintegrated due to the prevailed data inconsistencies, preventing the utility of an integrated analysis over large number of experiments. Our methodology is specifically designed to tackle this issue by creating a consistent data set across the experiment and platform boundary that facilitates efficient data exploration. 

Similar efforts that directly integrating publicly available data across experiments do exist, e.g., M$^{\textrm{3D}}$ \cite{Faith2008} and Genevestigator \cite{Hruz2008}. Centering only on the data generated by single platform (mostly Affymetrix), these efforts focus mainly on resolving the issue of inconsistently normalized data and on manual experiment metadata curation. The platform restriction, although simplifies data collection and normalization tasks, limits the amount of data can be integrated into the compendium. The methodology does not impose such a limitation, instead tries to incorporate as much data as possible by integrating data across different platforms.

To achieve our goal, we face two extra challenges, the data collection and the data normalization and integration. When compared to that of the single-platform compendium, the data collection is more complicated in two aspects. First, there lacks of standard formats to report the raw expression values measured on various platforms.  Especially for dual-channel platforms that probe expression values for two biological samples at a time, it is not simple to identify and extract the desirable data for each channel from data table of variant formats. Second, as two samples are hybridized on an array, the corresponding sample of the data generated in each channel must be correctly identified to guarantee that the proper experimental metadata information is used for annotation that is crucial for downstream data analysis and result interpretation. To this end, a semi-automatic raw data parsing subsystem capable of handling data format heterogeneity and extrapolating correct data sample associations is developed. In a majority of cases, experiment data available online are handled automatically by the system, avoiding time-consuming human intervention. When necessary, it do can incorporate extra information obtained from manual data inspection to extract data. The system enables us to process a large number of heterogeneous data efficiently. 

To handle the data generated from different types of platforms also complicates the data normalization and integration. In our system, platform specific normalization schema is utilized to handle particular issues associated with each type of platform, for example, loess to correct dye bias in dual-channel arrays, quantile normalization to reduce the variance among replicates for single-channel platform. Different types of platform produce different expression measurements. Single-channel platform generates absolute intensity value for each gene. Whereas, for dual-channel platform, log-ratio is generally preferred as calculated between the intensities obtained from the same sport, it effectively removes undesirable spot and array effects from the data. The log-ratio has been shown to improve consistency among results obtained across different microarray types \cite{Shi2006}, hence it is adopted as the type of the expression data for our compendium. To calculate log-ratio for all data, the concept of contrast is introduced, in which one sample designated as the test is compared with the other sample as the reference to reveal differentially expressed genes. For a dual-channel array, a contrast is naturally defined between samples hybridized to it. For an experiment using single-channel array, one or more reference samples are manually chosen based on the platform used and the nature of the experiment. The log-ratio can then be effectively calculated for each contrast with a clear biological meaning representing the observed gene expression changes induced by either the external perturbation(s) or the internal differences between samples. Furthermore, for experiments using single-channel platform, it has been shown that the log-ratios calculated between samples from the same batch (experiment and platform) effectively removes undesirable batch-effects \cite{Luo2010}.
 
The gene expression data are useful only when the accompany sample information is available.  We have also taken great care to provide an extensive formal contrast annotation and associated that with higher level condition ontology. As our compendium data are the log-ratios representing gene expression changes, our annotation focuses on specifying the differences between the pair of samples of a contrast. This is different from that of the single-platform compendium aiming to specify the complete information of each sample. 

The methodology described here enables us to create a cross-platform expression compendium. Through integrating the data across different platforms, the methodology has two advantages compared to the existing single-platform one.  First, it enables the creation of such a compendium that incorporate much more data,  hence providing a more complete expression landscape of a species. Second, it enables to create a sizable compendium for species which there is no dominant platform, such as Affymetrix. Utilizing this methodology, we successfully created three bacterial compendia for {\it Escherichia coli}, {\it Bacillus subtilis}, and {\it   Salmonella enterica} serovar Typhimurium.  The \textit{E. coli} one is the largest currently available, and the last two are unique. The details of these bacterial compendia are described in chapter \ref{ch:colombos}. Furthermore, the methodology has been adapted to construct compendium for eukaryote. A proof-of-concept compendium for \textit{Zea mays} (chapter \ref{ch:magic}) has been created, providing high data consistency by incorporating a complete re-annotation of the existing platforms based on the latest genome release and the extended annotations reflecting the complex structure and life style of a plant.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Keep the following \cleardoublepage at the end of this file, 
% otherwise \includeonly includes empty pages.
\cleardoublepage


% vim: tw=70 nocindent expandtab foldmethod=marker foldmarker={{{}{,}{}}}
