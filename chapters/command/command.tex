\chapter{The cross-platform compendium creation methodology and COMMAND 
system}\label{ch:command}
\chaptermark{Command}

\ldots

\instructionsintroduction


\todo(COMMAND: glossary ...)
GEO
ArrayExpress
SOFT
CEL
CDF
NCBI
EBI
MAGE-TAB
IDF
SDRF
ADF
MGED

\section{Introduction}

Microarrays are one of the main technologies for large-scale transcriptional
gene expression profiling.
%
To promot data sharing, scientific journals generally require the deposit of
these high-throughput experiments in public databases, such as Gene Expression
Omnibus (GEO) \cite{Barrett2011} or ArrayExpress \cite{Parkinson2009}, upon
publication.
%
These databases are an extremely rich source of information, containing freely
accessible data for thousands of experiments and a multitude of different
organisms, and in theory provide an opportunity to analyse gene expression of a
particular species at a global level.
%
They hold the potential to expand the scope of any smaller scale study: mining
the information contained in such databases offers molecular biologists the
possibility to view their own dedicated experiments and analysis in light of
what is already available.
%
So far however, this wealth of public information remains largely untapped
because these databases do not allow for a direct and integrated exploration of
their data.
%
The opportunity of combining all public experiments for a single organism has
not been explored due to practical issues that can ultimately be attributed to
the large heterogeneity inherent to microarray data.
%
Data sets originate from different experimenters or labs and microarrays do not
constitute a uniform technology.
%
Multiple microarray platforms exist and are manufactured in different ways.
Even for similar platforms, protocols for sample preparation, labelling,
hybridization and scanning can vary greatly.
%
Although community standards specifying the mandatory minimal experimental
information accompanying each dataset (e.g., MIAME \cite{Brazma2001}) have been
long established, the lack of the requirements \cite{Brazma2009} imposed
regarding the format of the platform descriptions and the expression
measurements, as well as the degree of preprocessing done on these values
further complicates the matter of experiment integration from a practical point
of view.


Despite such difficulties, several initiatives exist to actively build
expression compendia from public resources.
%
Most existing compendia can roughly be divided in two groups \cite{Fierro2008}:
those that directly integrate single-platform experiments, and those that
indirectly integrate cross-platform experiments.
%
Combining data from a single platform makes the in-between experiment
normalization and probe mapping relatively straightforward, so that the
quantitative measures of gene expression can be analysed directly across
experiments.
%
Most single-platform compendia databases, such as for instance
M$^{\textrm{3D}}$ \cite{Faith2008}, or the commercial Genevestigator
\cite{Hruz2008}, focus on Affymetrix, one of the more robust and reproducible
platforms \cite{Bammler2005, Irizarry2005}.
%
Combining data from different platforms, even to the extent of combining data
from single- and dual-channel microarrays, is generally done by indirect
meta-analysis as opposed to directly integrating the actual expression values:
one first applies the desired analysis procedure (e.g., identifying
differentially expressed genes, clustering gene expression profiles, etc.) on
each single data set within the compendium separately, and subsequently
combines the derived results.
%
These compendia are often topic-specific, collecting all publicly available
experimental information related to a subject matter of interest.  ITTACA
\cite{Elfilali2006} and ONCOMINE \cite{Rhodes2007}, for instance, focus on
cancer in human; Gene Aging Nexus \cite{Pan2007} on aging in several species.
%
There are exceptions though, such as the large ATLAS \cite{Kapushesky2010}
initiative from ArrayExpress.


The compendia created by directly integrating data across experiments have the
advantage of retaining actual expression values, which broadens the scope of
potential analysis procedures compared to indirect meta-analysis.
%
Most of such compendia center on eukaryotic organisms, for which considerable
amounts of data are available.
%
Relying on only one platform can still lead to sizeable compendia with a broad
scope in condition content, such as the human compendium constructed based on
the Affymetrix U133A platform with over 5000 samples \cite{Lukk2010}.  However,
for most species (e.g., \textit{Zea mays}), no single platform has such a
dominant role.
%
The worst is prokaryotes.  Even for model organisms such as {\it E. coli}, much
less data is available on individual platform and a significant portion of the
data is missed out when considering only one platform.


To have the advantage of the direct integration, while not being limited to a
single platform, we have devised methodology that directly combines expression
data across platforms and experiments.
%
The methodology has enabled us to create comprehensive compendia incorporating
most high quality public data covering a broad range of experimental conditions
as well as extensive types of biological samples.
%
To facilitate the creation of new compendium and the curation of the existing
ones, a software system COMMAND (Cross-platform cOMpendium MANagement Desktop)
is also developed, utilizing web browser as the front-end and our methodology
at the back-end.




\section{Methods}


\todo{COMMAND: reminder to be commented}
\textbf{REMINDER: Hurdles to create cross-platform exp compendium}

There are \textbf{three main hurdles} preventing direct data integration across
platforms and experiments.
%
\begin{itemize}
%
\item[exp-data] The lack of a consistent format to report expression data
  prevents an automatic and computerized data retrieve.
%
  The heterogeneity of microarray platforms hampers direct integration in two
  aspect.  The possible probe sequence differences of a gene across different
  platforms questions the compatibilities of the obtained measurements.
%
\item[metadata] The metadata describing biological sample and experimental
  conditions is provided as free text which is often incomplete and lack of
  consistency.  Standards like MIAMI \cite{Brazma2001, Brazma2009}, although
  promote human interpretation of experimental conditions, do not facilitate
  computational analysis.
%
\item[normalization] The different preprocessing algorithms employed by
  different experiments further complicates the data compatibility.
%
  The log ratio calculation, capable of removing certain technical variations
  from the normalized data, inherently improves the data consistency across
  platforms and experiments \cite{Shi2006, Shi2008}
%
\end{itemize}



\subsection{Cross-platform expression compendion}

Our expression compendium is essentially an organism-specific matrix of
expression values derived from publicly available microarray experiments which
are homogenized to make them comparable.
%
The rows of a compendium correspond to the known genes of the organism in
question constructed based on the corresponding RefSeq file at NCBI
\cite{Pruitt2007}.
%
Uniquely, each column of it is a `\textit{contrast}', which does not represent
single biological sample, but in fact always represents the differences between
a pair of samples, one as test and the other reference.
%
Consequently, the expression values themselves are calculated as expression
log-ratios.
%
Converting absolute measurements of expression into expression changes is the
principal means for rendering expression values comparable across platforms and
experiments.
%
Relative expression calculated intra-experiment/platform (i.e. between two
conditions measured in the same microarray experiment using one platform)
negates much of the platform and experiment specific variations that makes it
impossible to reliably compare the absolute quantities reported in different
experiments \cite{Shi2006}.


\subsubsection{The database schema for expression compendium}\label{sec:command-db-schema}
%% \todo{COMMAND: to be finished with a abstract schema in figure,
%%   Ref. MIAME2001, MIAMEPlant2006}

\begin{figure}
  \centering
  \includegraphics[width=1\textwidth]{DB_schema.pdf}
  \caption[The database schema for expression compendium]{ \textbf{The
      database schema for expression compendium}.
  }
  \label{fig:comp-db-schema}
\end{figure}

The compendium is stored in a relational database (MySQL). A abstract schematic
representation of various data content and their relation is shown in Figure
\ref{fig:comp-db-schema}.
%
The schema is separated into two parts. The contents in the upper part form the
data of a compendium, and are publically accessible.
%
The contents in the lower part are the data collected to build compendium, they
are used to create a new compendium and curate existing ones, hence are only
accessible internally.

Here we provide a brief explanation for each entity in the schema.
%
A gene expression \textit{experiment} is composed of a set of related arrays
that are designed to obtain expression data in order to answer a biological
question.
%
An \textit{array} is one microarray in an experiment on which the biological
sample(s) are hybridized and the gene expression measurements are generated.
%
A \textit{hybridization} refers to individual sample that is labeled and
hybridized on an array.
%
For an array using single-channel platform like Affymetrix, there is only one
hybridization per array.  Whereas there are two hybridizations, one per
channel, for the array using dual-channel platform.
%
A \textit{platform} denotes a specific microarray chip design and the
corresponding probe annotation information, such as, the probe sequence, the
physical layout, the target gene, etc.
%
The \textit{raw data} are original measurements of each probe grouped by
hybridization.  Three types of data are accepted, the raw expresssion value,
the background intensity, and the background corrected expression value.
%
A \textit{sample} refers to each individual biologcal sample that is used in
an experiment.  Biological duplicates are treated as different samples.
%
Note that a hybridization is an instance of a sample that is hybridized to a
specific microarray chip.  When the same biological sample are hybridized
onto several microarrays, they are refered as one sample but different
hybridizations in the compendium.
%
A \textit{contrast} as defined in previous section, specified a comparsion
between a reference sample and a test sample.
%
The \textit{norm data} are log-ratios, each of which represets the expression
variation of one gene under one contrast in which the gene's expression level
in the test sample are compared with that of the reference sample.
%
We use a relaxed defintion for \textit{gene}. It includes not only the protein
coding sequences, but also other sequences that are expressed and functioning,
given their expression level can be quantified.
%
There are also three supplementary entities, the article, the contrast
annotation, and the gene annotation.
%
The \textit{article} stores the publications related to an experiment.
%
The \textit{contrast annotation} contains, for each contrast, the sample
attributes and the experimental factors that differ between the corresponding
reference and test samples.
%
The \textit{gene annotation} stores the functional annotation of each gene
collected from external sources, including the metabolic pathway, Gene Ontology
annotation, transcriptional regulation, and the transcription unit.


\subsection{The compendion creation methodology}
\label{sec:colombos-comp-method}

\begin{figure}
  \centering
  \includegraphics[width=1\textwidth]{COMMAND_Workflow.pdf}
  \caption[Cross-platform expression compendium creation methodology]{
    \textbf{Cross-platform expression compendium creation methodology}.
    From left to right, three box represent three main steps of the 
    cross-platform expression compendium creation methodology. The 
    corresponding functionalities are specified in the rectangles inside of 
    each box. Below the box shown the snapshots of the corresponding COMMAND 
    interfaces.}
  \label{fig:command-workflow}
\end{figure}


The cross-platform compendium creation methodology is composed of three major
steps (Figure \ref{fig:command-workflow}), namely, data collection, experiment
annotation, and data homogenization.
%
First, in data collection step, the expression data and the accompany
(experiment and platform) information are retrieved from the online
repositories, overcoming the issue of the prevalent data representation
discrepancies.
%
Next in annotation step, the contrasts are constructed by assigning a pair of
samples, one as reference and the other as test. And the experimental condition
differences between the corresponding samples are curated manually for each
contrast and specified using a set of in-house controlled vocabularies.
%
At last, the raw expression data are normalized and converted into log ratios
based on the sample contrast association in data homogenization step, creating
a cross-platform expression compendium.
%
In the following sections, we will explain the individual step in detail.



\subsubsection{Data collection}

%% [Colomobos (v1) paper]
%%
%% The first step is the retrieval of microarray experiments and associated
%% platforms from Gene Expression Omnibus (GEO) and ArrayExpress. Representation
%% discrepancies prevalent in experimental data directly obtained from online
%% databases are systematically removed and the resulting data are then stored as
%% available in a uniform format. ‘As available’ does not necessarily equate to
%% raw scanner output, since there are no MIAME reporting standards regarding the
%% measurement units of expression \cite{Brazma2001, Brazma2009}. Often raw
%% intensities are not provided in the public databases (especially for older
%% experiments), and only already processed data are reported.

%% General issues to be covered
%% %
%% \begin{itemize}
%% \item free text for experimental information [metadata]
%% \item lack of standard format to report platform probe specification,
%%   sequence information is not always provided [probe specs]
%% \item raw data is not always required, if provided, lack of standard
%%   format to report raw data [raw data]
%% \item lack of platform type specification
%% \item the number of channels specification is provided in GEO Sample 
%%   record and can be derived from SDRF file from ArrayExpress [raw 
%%     data] 
%% \end{itemize}


%% Summary
To create compendium, the publically available expression data need to be
retrieved from online repositories, such as, Gene Expression Omnibus (GEO)
and ArrayExpress.
%
Although community guidelines such as MIAME \cite{Brazma2001} exists, the
failure to provide a standard format to represent information of an expression
experiment causes widespread represetation discrepancies among the data
deposited in those online repositories.
%
The data discrepancies coupled with the colossal amount of the available data
in those repositories makes the systematical expression data retrieval a
daunting task.
%
To make this task viable, we develop a semi-automatic workflow designed to
tackle data discrepancies and automate various processing procedures to handle
the huge volume of the data.

Here, we first explain what data are available, then the different sets of data
to be retrieved, at last, for each set of data, discuss the data issues related
to its retrieval and our strategy to tackle them.
%
\todo{COMMAND: recheck and adapt it when finish this section}



\paragraph{Data content available at online repositories}\label{sec:command-data-content-online}

There are two main public online repositories, GEO and ArrayExpress.
%
Although both are MIAME-compilant, they use different files and format to store
experiment information.

%% http://www.ncbi.nlm.nih.gov/geo/info/overview.html
%% http://www.ncbi.nlm.nih.gov/geo/info/soft2.html
%
In GEO, there are two sets of data for an experiment, the \textit{GEO Series}
(GSE) containing the original submitted data of an experiment and the
\textit{GEO DataSet} (GDS) containing curated data of an experiment.
%
To create compendium, the GSE data is used.
%
The GSE data of an experiment is stored in a standard simple line-based plain
text format, called SOFT (Simple Omnibus Format in Text), one SOFT file per GSE
in compressed (gzip) form.
%
The file contains one or multiple instances of three types of records, Series,
Sample, and Platform.
%
The \textit{Series} record contains general information about various
aspect of an experiment described in free text.  The references to the
corresponding Sample and Platform records are also provided.
%
The \textit{Sample} records, each of which refers to one microarray chip
used in an experiment, is composed of two sections.
%
The first section provides free text descriptions about biological samples
hybridized to the microarray, including information about sample attributes and
experimental factors, which is crucial for compendium data exploration and
interpretation.
%
It also provides the reference to the related Platform record.
%
In the second data section, the gene expression abundance measurements are
reported for each probe of the microarray in a tabular form. Often the values
reported here are processed data instead of the original probe measurements
desirable for creating compendium.
%
The \textit{Platform} record describes the microarray chip used to measure gene
expression, including a free text description of the array and its probe
annotations in a tab-delimited table.
%
Specific about the required content in various records and their sections, the
SOFT format does not impose strict requirements about the representation of the
data.
%
Consequently, the format used to report the expression values and the probe
annotations various across experiments and platforms respectively.
%
Additionally, for most experiments, raw data files are available as
supplementaries providing an alternative source to obtain desirable gene
expression measurements.
%
Note that the submission of the raw data file is not mandatory in GEO.
%
Hence, many experiments do not have supplementaries.
%
In GEO, a unique access id is assigned to every record.
%
The `Series', `Sample, and `Platform' record in GEO directly corresponds
to the experiment, array, and platform object respective in our schema
(Fig. \ref{fig:comp-db-schema}).


%% www.ebi.ac.uk/miamexpress/help/index.html
%% www.ebi.ac.uk/miamexpress/help/datafile_help.html
%% 
%% www.ebi.ac.uk/fgpt/magetab/help/
%% 
%% www.ebi.ac.uk/training/online/course/arrayexpress-submitting-data-using-mage-tab
%% tab2mage.sourceforge.net/docs/magetab_docs.html
%%
%% www.ebi.ac.uk/arrayexpress/help/accession_codes.html
%
In ArrayExpress, the experiment data is stored in the MicroArray Gene
Expression Tabular (MAGE-TAB) format.
%
In this format, 4 files capturing different set of information are used,
namely, the Investigation Description Format (IDF) file, the Sample and Data
Relationship Format (SDRF) file, the Array Design Format (ADF) file, and the
raw and processed data files (normal packed into one zip file).
%
Similar to the Series record in GEO SOFT format, the \textit{IDF} file provide
an overview about an experiment.
%
Although the MGED ontology terms are used, they specify only the type of the
information.  The content, however, is still described in free text.
%% %
%% Although linked to the corresponding SDRF file, it provide no direct
%% references to the samples, the platforms, or the data files.
%
The \textit{SDRF} file describes the sample characteristics and the
relationship between samples, arrays, data files.
%
The content is presented in a tabular form, in which each record (row)
describes information about a biological sample hybridized to a specific
microarray chip to measure gene expression levels. 
%
Hence, a record in SDRF file corresponds to a compendium hybridization
object (Fig. \ref{fig:comp-db-schema}), as the data measured on a
dual-channel microarray is described in two records, one for each channel.
%
For each record, the references to the corresponding platform are
provided.
%
One caveat about the SDRF file is that its format is not strictly defined,
hence can vary among experiments.
%
The \textit{ADF} file provides information for a microarray platform,
including both the descriptive information and the probe annotations.
%
At last, the expression values are reported in the raw and/or normalized
data files.
%
In ArrayExpress, the unique access id is given only to experiment and
platform, which directly corresponds to the experiment and platform object
in our schema (Fig. \ref{fig:comp-db-schema}).
%
However, there is no explicit record in ArrayExpress that corresponds to
the compendium array object.  Additionally, although each record in SDRF
file corresponds to a compendium hybridization object, without a unique
reference, this information is only indirectly accessible though the
correponding experiment.
%
%% In ArrayExpress, the unique access id is given only to experiment and
%% platform. The sample or the hybridization information are only
%% indirectly accessible though the correponding experiment.


Note that the raw data files reported in GEO and ArrayExpress are
generated directly from the image of an microarray using various image
scan softwares.
%
As there is no standard expression data report format, each software uses it
own one.
%
Additionally, often allow to be used alone, both the annotation and the
expression value(s) are provided for each probe in those files.






\paragraph{Data retrieval goal}\label{sec:command-data-goal}

There are three sets of information to be extracted, the experiment metadata,
the platform specification, and the raw expression values.
%
\todo{COMMAND: need to be explained in the introduction}
%
The experiment metadata describing the sample attributes and experimental
factors is essential for exploring the data and interpreting the observed
expression variations.
%
As the expression value is measured by individual probe of a microarray, the
measurements reported are for each probe instead of gene.
%
The platform specification contains the probe-to-gene correspondence
information that is essential for converting probe measurements into gene
expression values.
%
%% Additionally, the platform specification also provides clues about the
%% number of samples hybridized on a microarray providing hints about the
%% expected amount of data to be retrieved.
%
The raw expression values are the foundation of the compendium.  The quality of
the compendium data depends directly on the raw data's quality.
%
To reduce the data inconsistency introduced artificially due to the different
normalization schemas used to process raw data, the original probe intensity
expression values without background correction are prefered, which is then
normalized utilizing an in-house pipeline to improve the consistency.
%
However, when that is impossible, the system do can incorporate processed data,
albeit with less consistency.
%
%% % [Colombos (v1) paper]
%%
%% Representation discrepancies prevalent in experimental data directly obtained 
%% from online databases are systematically removed and the resulting data are 
%% then stored as available in a uniform format. 
%% %
%% `As available' does not necessarily equate to raw scanner output, since there 
%% are no MIAME reporting standards regarding the measurement units of expression 
%% \cite{Brazma2001, Brazma2009}. 
%% %
%% Often raw intensities are not provided in the public databases (especially for 
%% older experiments), and only already processed data are reported. 
%
Except for extracting original expression data, equality important is to retain
proper value-to-probe mapping and value-to-hybridization association
%% the proper expression value to probe mappings and the expression values to
%% hybridization association. 
(see Fig. \ref{fig:comp-db-schema}).
%
The former is required to converted the probe expression values into the gene
expression value.
%
The later links the expression value with the corresponding biological sample,
consequently becoming associated with related sample attributes and
experimental factors to facilitate data exploration and interpretation.

%% % Moved from exp data extraction section
%% %
%% It seems the solo goal of expression value extraction is to collect desirable probe
%% expression values. However, there is a hiden task that is to assign one expression
%% value to the apropriate hybridization (or a biological sample with the correct
%% label). 
%% %
%% Correctly connecting the expression value to the corresponding sample is
%% crucial, as the associated sample attributes and experimental factors are the
%% drive of the observed expression abundance, and are important for data
%% exploration and interpretation.


\paragraph{Experiments retrieve, filtering, and data downloading}

Although start as gene expression database, GEO and ArrayExpress are
extended through the years to also include functional genomics data
other than gene expression data, e.g., ChIP-chip experiments.
%
Furthermore, certain types of expression data, e.g., those of the
cross-species gene expression comparison studies, are not interesting
for a compendium specific for one species.
%
Hence, it is preferable to check and exclude those data before
continue.

To maximally automate this process, the programmable access interfaces,
Entrez Programming Utilities (E-Utils) of GEO and REST query API of
ArrayExpress, are utilized to retrieve the basic metadata of all
experiments belong to a given species.
\todo{COMMAND: is there a experiment type parameter?}
% http://www.ebi.ac.uk/arrayexpress/help/programmatic_access.html
% http://www.ncbi.nlm.nih.gov/geo/info/geo_paccess.html
%
Note that GEO database, as the eariliest of two, is served as our primary
data source, whereas ArrayExpress supplements GEO by providing extra data
that are not available in GEO.
% GEO 2001, ArrayExpress end of 2002
%
Upon obtaining the list, the existing experiments are removed.  For the
experiments common in both repositories, only the corresponding GEO record
is kept.
%
%% Note that ArrayExpress regularly incorporates GEO experiments into it, 
%% As those data are obtained from GEO, they are skipped when handling
%% ArrayExpress data.
%% %
The remaining experiments is sifted through filters based on a set of
in-house collected key phrases to remove undesirable experiments, e.g.,
ChIP-chip data, cross-species comparison data, etc.
%
The filtering is rather conservative to avoid excluding experiments by
mistake.
%
The remaining experiments are then reported as new ones.  A human
inspection on this list of new experiments is recommended, although
not mendatory. 
%
When definite, the corresponding data file(s) can be automatically
downloaded from the relevant online repository for data extraction.

%% After retrieving a list of experiments, the existing ones are removed. So are
%% the duplicates between different repositories.
%% %
%% A keyword based filtering step is applied to further remove experiments other
%% than those measure gene expression, generating a clean list of new experiments
%% to be processed further.
%% %
%% Next, the data files of the new experiments are retrieved from the
%% corresponding online repositories, and the in-house parsers are applied trying
%% to extract experimental data and gene expression values automatically.  This is
%% a repository specific procedure.



\paragraph{Experiment metadata extraction}
%
The experiment metadata includes the basic experiment information
(e.g., name, description, publication, etc) and the sample attributes
and experimental factors.
%
As mentioned before, instead of using controlled vocabulary, this set
of information is often described using free text, which lacks
consistency as often one concept can be described in multiple ways.
%
Additionally, most often the metadata provided with the experiment is
incomplete, the missing information need to be manually extracted from
the related articles.
%
The inconsistency and incompleteness of experiment metadata renders a
computerized data extraction impossible.  Instead, a manual curation
(in the annotation step) is required to analyze and standardize these
information into a computable format to facilitate data exploration.
%
Consequently, this information is extracted in its original form without
modifications.
%% %
%% We delibrately avoid using any text mining technology, as none can
%% void a manual inspection which is as time consuming as manual
%% curation.

%% \textbf{ATLAS2013}
%% - experiment metadata, experimental condition \\
%% - sample attributes and experimental factors (i.e. conditions under
%%   study) 

%% \textbf{M3D} The fourth obstacle is the incompleteness and
%% inconsistency in the curation of metadata describing the details of
%% each experimental condition. Each expression profile run for a
%% given species can have a different genetic background, media,
%% growth conditions and any number of chemicals, which might have an
%% effect on the cell’s expression. Such data is fundamental to the
%% meaningful interpretation of expression data. Even when provided,
%% this metadata is found as unstructured prose in the database
%% deposit or in the methods sections of each publication. Ideally,
%% this metadata would be collected in a computable format with
%% uniform units across all laboratories. Although standards like
%% MIAME (10) promote the human interpretation of experimental
%% conditions, the standard is unevenly applied and it does not
%% facilitate computational analysis

As GEO and ArrayExpress use different formats, two parsers, one for each
database, are developed to extract the metadata.
%
For GEO, the required metadata is gathered from the `Series' and the
`Sample' records of an experiment.  The direct correspondance between
those GEO records and the compendium objects makes this extraction
straightforward.
%
Whereas for ArrayExpress, the metadata extraction from the IDF and SDRF
files are not so simple.
%
The first issue is the data content inconsistency in SDRF file, as it has
no strictly defined format.
%
The data can be submitted into ArrayExpress using different methods,
e.g. MIAMExpress, MAGE-TAB, etc, generating compatible but not exact the
same set of information.
%
For exmaple, `Hybridization'\footnote{Noted that this `Hybridization' is
  similar to the GEO `Sample' record, which refers to one microarray chip
  used in an experiment and corresponds to the compendium array object.
  %
  This is different from the compendium hybridization object that
  corresponds to one channel of an array.}, which is mandatory in
MIAMExpress, is replaced by `Assay' in MAGE-TAB, which is not obligatory
and often missing.
%
%% A rather specific case, hence removed
%%
%% For exmaple, sometimes, the required `Array Data File' column has no
%% content. This makes it very difficult to identify the corresponding raw
%% data file of a hybridzation, and indirectly complicates the identification
%% of the hybridizations come from the same dual-channel microarray chip.
%
Second, there lacks of an object corresponds to the compendium array
object, which uniquely refers to one microarray chip and bundles the
corresponding channels together.
%
For a dual-channel microarray, it is crucial to pair two channels
together, as their raw data are often jointly normalized.
%
Recall that each SDRF record (row) corresponds to one channel of an array,
this relation between channels then needs to be derived indirectly from the
data content.
%
Due to the data inconsistency, multiple data sources, e.g. `Array Data
File', `Scan Name' or `Hybridization Name', are utilized in this process,
depending on their availability.
%
When this relation is identified, the corresponding SDRF records are
paired and assigned as different hybridization(s) of an compendium array
object.
%% %
%% As no channel id specification is provided, the one labeled with `cy5' is
%% artificially assigned as `ch1' whereas `cy3' as `ch2'.
%% %
%% % Unique ID issue is skipped, as it is not mandatory to obtain it from
%% % SDRF file content
%% %
%% Due to the data inconsistency, multiple methods has been developed to
%% obtain a unique ID for each Array from different sources available in SDRF
%% file.
%% %
%% % NOTE: the `label' is provided in almost every exp, so not an issue
%
When the system fails to identify this relation, a manual inspection is
required.











\paragraph{Platform specification extraction}
%
The platform speficication includes both the basic platform information and
the probe annotaitons.
%
The basic inforamtion, including name, description, manufacturer, etc, are
directly extracted without changes.
%% %
%% However, the number of channels applicable for a platform is missing.
%% %
%% This information is crucial for computerized raw data extraction, as
%% it hints at the amount of data expected to be extracted.
%
The probe annotation is always presented as a table, nevertheless each platform
has its own format that varies in the amount of content (number of columns),
the column naming convention, and the content of individual column.
%
Idealy, the probe sequences should be provided so that a homology search
against the lastest gene sequences using BLAST \cite{Altschul1997} can identify
the up-to-date target gene for each probe.
%
However, although required by MIAME\cite{Brazma2001} standard, this information
is missing for the majority of the platforms.
%
Alternatively, the target gene can be identified by other information, namely,
locus tags, alternative gene tags, or common gene names.
%
But the lack of a consistent format makes it hard to identify the corresponding
column providing those information.
%
Even worse, sometimes the relevant information is embedded in a column in which
multiple contents are specified in a complicated structure.
%
Furthermore, the content inconsistency exists that different types of
information are found in one data column.
%
For example, locus tag, gene name, or other tags are often found to be used
alternatively in one data column titled 'ORF'.


Due to these data issues, a fully automated platform data extraction is not
feasible.
%
Hence a user-guided semi-automatic three-step procedure, which collects a
standardized set of information for every platform, is developed.
%
In the first step, the platform information is separated from other
information.
%
An in-house dictionary storing common column names, the corresponding most
probable content type, and the most common extraction method is utilized to
automatically mark out the candidate data columns.
%
Next, a manual inspection is required.
%
The content and the data extraction functions of the automatically marked
columns are checked, and the adjustment is made when necessary to guarantee the
contents are properly extracted.
%
The other columns are checked only when necessary information could not be
obtained from the marked columns.
%
In the last step, the data are automatically extracted, the target genes are
identified, and the information is imported into the corresponding database
table.
%
The data extraction functions are implemented as a plugin based system that
automates the content parse process.  Currently, 9 plugins together with the
direct `copy' option are capable of handling every platform included in the
existing compendia, and the system can be easily extended by adding new
plugins.
%
When identifying target gene without sequence information, the aforementioned
content inconsistency is handled by an integrated search over multiple
information sources in strict order of preference, locus tag, alternative gene
tag, and then common gene names, based on its reliability.


Integrated with these functionalities, the probe annotation parsing subsystem
is flexible and powerful to handle data represented in various formats.
%
For many platforms, the probe annotations can be parsed with the automatically
detected settings without alteration.
%
% This greatly improves the productivity.
%
However, the manual inspection is still required to guarantee the correctness.
Besides, the inspection can identify extra information in the data.  As often
embedded in a complex content format requiring a specialized data extraction
function to handle, this information cannot be identified automatically.

%% % [Colombos (v1) paper]
%% %
%% At this stage probes are also mapped in a platform-specific manner to a unique 
%% list of genes which is constructed based on the organism's RefSeq file at NCBI 
%% \cite{Pruitt2007} and which corresponds to the rows of the final compendium. 
%% If probe sequences are available or can be obtained from the platform 
%% description, the mapping is driven by sequence homology searches using 
%% BLAST \cite{Altschul1997}. 
%% %
%% If not, a probe's target gene is identified by other probe info, namely -and in 
%% order of preference: locus tags, alternative gene tags, or common gene names.


%% The platform type indicates the expected amount and the types of expression
%% values that can be obtained by a platform.  For example, for Affymetrix
%% platform, probe or probe set level intensity values are expected.  Wherease
%% for cDNA platform, the probe level intensity and background values can be
%% obtained.
%% 
%% % We checked the cases where cDNA platform but with one channel data per
%% % chip (db_scripts.sql, QUERY 1).  They are rare and can be handled case by
%% % case.  Hence the rules above can be seen as generally applicable.





\paragraph{Expression value extraction}

\textbf{
  \begin{itemize}
  \item GEO SOFT format issues
    \begin{itemize}
    \item channel count is provide for each sample!
    \item data type identification 
    \item automatic data conversion
    \item channel data to sample mapping
    \end{itemize}
  \item GEO supplementary files
    \begin{itemize}
    \item diff format issue
    \item file to Array record correspondance identification
    \item channel data to sample mapping
    \item probe mapping
    \end{itemize}
  \item ArrayExpress
    \begin{itemize}
    \item channel mapping
    \item raw data file platform information handling
    \end{itemize}
  \end{itemize}
}

\textbf{
brief about single channel, then focus on dual-channel \\
%
- simple targets, an automated system is possible \\
- complex possibilities (many possible types, no standard column 
  naming convension), semi-automated system
}

%% % 3 hybrids per array is skipped as it is rather rare, and as the Genomic DNA
%% % is used in the third channel, the data are removed from subsequence
%% % processing.
%% Although maximally, 3 hybridizations, labeled with the dyes of
%% different colors, per array has been observed in some Salmonella
%% experiments, this setup is rather uncommon.


As mentioned previously, the expression value extraction has two goals, to
extract desirable raw expression data and to retain proper value-to-probe
mapping and value-to-hybridization association.
%
The expression data can be extracted from three different sources, the GEO
Sample record, the GEO supplementary file, and the ArrayExpress raw data files.
%
As each source has its own issues, we will explain the issues associated with
each individual data source and our handling strategy, first the GEO Sample
record, then the GEO supplementary file, and at last, the ArrayExpress raw
data files.
%
Note that the exrpession value extraction from the single-channel microarray
data is easier than from the data generated by a dual-channel microarray, as
the value-to-hybrization association is straightforward, given that only one
hybridzation exists.
%
To apprieciate the full complexity of the expression value extraction task
and to demonstrate the full capacity of our strategy, we assume that the data
to be handled are generated by a dual-channel microarray.  
%
Wherease the single-channel microarray data can be handled by the same
strategy leaving out the value-to-hybrization associations extraction part.

\todo{COMMAND: a general description of the exp data parsing subsystem, to be
  adapted and removed}
%% {Our data retrieval system aims to extract raw expression intensity values by
%% applying diverse strategies on various types of aforementioned discrepencies.
%% %
%% Additionally, to enable handling the colossal amount of data available, we
%% integrated those strategies into a software system which applies them
%% automatically to improve productivity.
%% %
%% Note that as certain discrepancies are difficult to handle even manually, our
%% system is semi-automative. Manual intervention is possible to handle difficult
%% cases wherein the automated procedure fails.  
%% %
%% Human inspection is not mandatory but recommended to guarantee the correctness
%% of the programmatically generated results.}


\subparagraph{\textit{Parse GEO Sample record}}

As explained previously, each GEO Sample record has a data section that reports
expression values in a tabular form.
%
Although only processed results are required to be reported here, often they
are accompanied by the corresponding raw expression values.
%
As part of the SOFT format, the proper value-to-probe mapping is guaranteed
through using the same probe id to refer probes across the corresponding Sample
and Platform records.
%
This simplifies the data extraction task. 
%
Hence it serves as our primary source for raw data extraction.

To successfully extract raw data, two tasks remain, to properly identify the
desirable raw expression value and to obtain correct value-to-hybridization
associations.
%
The first task is hindered by several issues. First is the lack of standard
data reporting format in Sample record.
%
Instead, the data table of the corresponding raw data file is often used.
%
The existence of a large number of raw data formats creates widespread data
representation discrepencies.
%
Although providing the data table header description is mandatory, given in
free text, it impedes an computerized data extraction.
%
Second, measuring two samples in one chip, it is crucial to identify the
channel association for each data to be extracted.  However this information
can only be derived indirectly through analyzing the column name.
%
At last, sometimes, only the background corrected values instead of the
original ones are reported.  As this correction may result in an increased
variance for lower, less reliable intensity levels \cite{Ritchie ME,
  Bioinformatics, 2007;23:2700-2707}, corrected values are not desirable.
%
When the corresponding background value is available, the uncorrected values
can be reconstructed from the data.  However, this requires that the system is
capable of automatically identifying and applying certain data conversions.
%
Note that since raw expression data are reported as numbers, there exists no
extraction issue caused by the complicated data format as seen in platform data
parsing.
%
For the second task, to obtain value-to-hybridization associations, the data
channel information needs to be paired with the hybridization information
specified in the Sample record.
%
Although specified as `ch1' and `ch2' in the Sample record, various types of 
specifications are used in the column names due to the data discrepencies. 


Given the above issues, we developed a semi-automatic raw expression value
extraction system.
%
The fully automatic extraction is triggered under a strict condition, in which
the raw expression value without background correction and the corresponding
background values are extracted, the channel information of each extracted data
is properly identified, and the value-to-hybridization associations are
corrected derived.
%
Such a strict condition guarantees that correct data are extracted by the
system.
%
When the condition cannot be satisfied, a manual inspection guided data
extraction is provided as a fallback.  Additionally, the supplementary file, if
exists, can also be analyzed to try to extract raw values.
%
The key of the automatic data extraction lies in the ability to
programmatically identify the data content type and the channel information
avoiding manual inspection.
%
Our solution is a rule based column name analysis subsystem that analyzes the name
of data column using pattern matching to identify required information.
%
Targeting the desirable data, the system focuses on identifying three types of
them, the raw expression intensity value ($I$), the background value ($BG$),
and the background corrected expression value ($I_{BG}$).
%
For each data type, the related columns are collected from the Sample records.
The patterns of the column names are then manully analyzed, and the rules to
recognize the data type and the channel related keywords are derived and used
in the system.
%
Next, the extracted channel keywords are checked against the hybridization
specification to obtain value-to-hybridization association.
%
It can handle three types of keywords, the direct channel specificaton (`ch1'
and `ch2'), the dye color specification (`cy3' for green and `cy5' for red),
and the dye frequence specificaion (`532' for green and `635' fro red).
%
The first type can be directly mapped to a hybridization.  Whereas, for the
last two types of keywords, the labelling information of a hybridization is
required.
%
After all columns are analyzed, the system checks those identified and
automatically discover and set the data conversion function when necessary.
%
At last, the aforementioned condiitons is checked against the information
identified by the system.
%
When succeed, the raw expression data are then automatically parsed.
Otherwise, the corresponding error is reported to highlight the issues for
manual inspection.
%
%% % NOTE, the conversion from (M,A) back to channel I data is abolished, as in 
%% % most case, the M,A values are processed ones, hence cannot be used to 
%% % calculate the raw intensity values. 







\subparagraph{\textit{Parse GEO supplementary file}}

When the Sample record does not provide deirable raw expression values, it is
still possible to extract them from the raw data file that supplements a
Sample record.
%
Note that designed to be self-contained, the raw data file includes not only
the expression value data, but also the probe annotations and various level
of meta information.
%
Except for the same issues that encountered when parsing Sample record, there
are other issues when parsing raw data files.
% * different file types
First, as mentioned before, there exists a large number of different formats
for raw data file, which varies on the mount of the meta information
available, the probe annotations used, and how the expression values are
reported.
%
Hence, each format might need a specific parser to handle it, and to automate
the process, the system should be able to automatically recognize the file
format.
% * probe matching
Second, as the raw data file is not part of the SOFT format, the probe
annotation specified in the file need not use the same probe id provided in
the corresponding GEO Platform record.
%
To improve the consistency across the experiments using the same platform,
the GEO probe id is prefered.
%
Consequently, the probe information need to be mapped to GEO probe id.
%
There is an additional complication as a raw data file format is not always
platform specific, hence the parse should be flexible to handle different
platforms.
%
% * file matching
At last, although required, the supplementary file corresponds to a Sample
record is not always reported in the Sample record, hence the system should
be capable of identifying it.


To guarantee the extraction of correct data with proper probe and
hybridization associations and to automate the parsing process, several
format specific parsers are developed to handle most popular raw data file
formats.
%
Currently, following formats are supported, GenePix Results format (GPR),
and Perkin-Elmer ScanArray format.  
%% NimbleGen Pair format removed below with Imagene, as similar to Imagene,
%% data are split into two files, one per channel
However the NimbleGene Pair format and Imagene format is not supported, as
manual intervention is required to handle them\footnote{The results of a
  dual-channel platform reported in these formats are split into two data
  files, one per channel.  It is often impossible to programmatically derive
  the file to hybridization association.}.
%
Each data format is thoroughly studied to develop the corresponding parser.
Similar to Sample record data parsing, the raw value data columns and the
corresponding value-to-hybridization associations are identified from column
names.
%
The focus of a parser is to provide format specific method to obtain accurate
value-to-probe mapping, which should also be flexible to handle different
platforms.
%
A greater accuracy is achieved by utilizing the well structured and accurate
parsed probe data of the corresponding platforms to search against the
standardized probe information provided in a raw data file.
%
A match could be identified in multiple manners applied in the order that
favors the most accurate one applicable, providing flexibility to handle the
amount of information varies across platforms.
%
As a consequence, to parse a supplementary raw data file requires that the
corresponding Platform record has been parsed with highest quality obtaining
as much information as possible for probe matching.


The supplemantary file is parsed by a semi-automatic three-step procedure.
%
Frist, the raw data file to GEO Sample record association is either obtained
from the Sample meta data or derived from the existence of Sample access id
in the name of the corresponding file.
%
Next, the format of the raw data file is identified by either the file
extension or its meta information signature.
%
%% When identified, the dedicated parser, created through a careful study of
%% that format, is utilized to extract raw expression values, pair the probe
%% information in a raw data file to the GEO probe id, and obtain correct
%% value-to-hybridization associations.
When identifiable, the corresponding dedicated parser is then utilized to
automatically extract raw values and assign them to the proper probes and
hybridizations.
%
For the file of unknown format, a manual data parsing is provided as an
alternative.
%
The new formats encountered are tracked, and a dedicated parser can be added
when necessary.
%
% This functionality exists in the backend code, but the interfaces in
% COMMAND still need to be developed .
%
To avoid the overcomplexity, the automated system only handles the case where
only one raw data file per Sample, and supports only the ASCII file and the
excel binary file (csv and xls).

%% To avoid the overcomplexity, we limited the case in which an automatic
%% parsing is applicable to one raw data file per array, and the file types to
%% either ASCII file or excel (binary) file.
%% %%
%% %% NOTE: the supplementary file parsing system only handles known format !!
%% %%       ( suppleRawDataFileParser.rb ) alternatively a manual parsing is
%% %%       possible through information stored in DB tables
%% %%
%% %%       Platform Rawdata Separation is used only in ArrayExpress !
%% %%
%% The system combines the dedicated parsers for widely used file format with
%% the column name based automatic system used to parse Sample record.  The
%% former guarantees the quality of the extracted data, and the later provides a
%% general framework that extends the system beyond the known format when
%% applicable.
%% %
%% For a data file of unknown format, the system only support those containing
%% only data table without meta information, as it is impossible to
%% programmatically separate meta information from data table section for an
%% known format.

%% However in vender specific file format, the expression values in these files
%% must be remapped to the corresponding probes specified in SOFT file to maintain
%% a consistent platform specification across multiple experiments. 







\subparagraph{\textit{Parse ArrayExpress raw data file}}


% Check apRawDataParser.rb
% 
% 1. automatically split platform columns from rawdata columns
%    (apPlatformColumnIdentifier)
%
% 2. column name based system to extract raw value
%
% 3. also Affymetrix ones are handled directly using affy parsers

% ----------------------------------------------------
%
% Part I: preconditions
%      - data file to compendium array object
%      - data to hybridization(channel) to sample assignment
% Part II: platform information handling and data separation
% Part III: raw data parsing
%
% ----------------------------------------------------

- identify the corresponding raw data file of an array
- labelling information inconsistency (channel mapping) 


In ArrayExpress, the expression values are reported in the raw data files
and normalized data file.
%
The raw data file is our focus as they provides the data desirable for
creating compendium.
%
In ArrayExpress, there exists two kinds of raw data files, the vender
specific data files, e.g. Affymetrix CEL file, Genepix GPR file, etc, and
the processed raw data files which is normally a reformated data file.
%
The former are used mainly for the experiments based on Affymetrix
platformss, and rarely for the experiments using GPR files.
%
The vender specific data files are handled either by dedicated procedure
(CEL file) or the format specific parsers developed to handle GEO
supplementary files (GPR file) with some adaptations.
%
The majority of the experiments, including most of those originally using
GPR files, report expression values using the processed raw data files,
which is a tab delimited text file contains only an adapted data table
without meta information.
%
Two kinds of changes are applied on the data table.  First, the platform
related part of the table, containing probe annotations, is replaced with
the corresponding annotations available in the ArrayExpress platform record.
%
Second, the column names of the expression data section of the table are
augmented with one extra information of different types.  The most common
types include but not limited to the original file format, the corresponding
`Hybridization Name' specified in SDRF file, or the original data file name.
%
Due to this specificity, it is possible to separate platform related data
from the expression value related data, and furthermore, recover the
original raw data column names in a majority of cases.
%
Below, we will first explain the general condition required before
proceeding to raw data extraction, then the data extraction from the
processed raw data file, followed by raw data file probe information
handling.


% Part I: preconditions
The raw data extraction is proceeded only when it is possible to link the
extracted expresssion values to the corresponding biological samples.
%
Note that the SDRF file parsing has successfully recontructed channels
relations and created the corresponding compendium array object.
%
However there still exits two missing links to connect raw data to sample.
%
The first link is to identify the corresponding raw data file for each array
object.  Generally, the `Array Data File' column in SDRF record specifies
this information.
%
When that is missing, we notice that the extra information added into data
columns sometimes provides clues, for example, when it matches the
hybridization name of a SDRF record.
%
In those cases, the system is capable of automatically identifying the data
file correspondence.
%
Whereas a manual inspection is required when the correspondence cannot be
identified.
%
The second link is to obtain the value-to-hybridization associations for the
data generated on dual-channel microarray platform.
%
As the dye color used to label a sample are specified in the corresponding
SDRF record, 


To this end, the same procedure used


This requires two conditions.



As which guarantees that
%


However promising, this does make raw data extraction less challenging
then previous two data sources. 
%
Similar to the handling of GEO supplementary file, a correct association
between raw data file and the Array object artificially created while
parsing SDRF file is required.
%
Yet when `Array Data File' information is missing in SDRF file, this
associations .

There are two conditions to safe guard a successful data extraction.
%
First, the raw data file must have been properly associated with the
articiailly created Array object.
%
Second, the proper value-to-hybridization association can be readily
derived from the data column names.
%
These conditions guarantee the interpretability of the extracted raw
values, as the corresponding biological sample can then be identified.
%

As the hybridization information are specified in SDRF file, whereas the raw
data files are provided separately, a proper file to hybridization
association is required to parse the

need to be identified





%
% Part II: platform information handling and data separation
% 


%% platform inform is extracted from raw data file
%% 
%% www.ebi.ac.uk/miamexpress/help/array_designs.html
%
%    'Reporter Identifier' and 'Reporter Name' columns are mandatory
%      - use position information to match is the ONLY method to be uniq,
%        if probe mapping needs to be identified !!
%
%      - 'Name' matches better than 'Identifier', but might missing in raw data file
%      - 'Identifier' needs to clean up to extract useful information
%      - 'Identifier' & 'Name' is not unique!!!  Especially for control probes.
%
%      - When original gpr file is used, impossible to use position as
%        'BLOCK' is replaced with 'metaRow' 'metaColumn', the mappings
%        only available in GEL array design file 
%
%    Reporter BioSequence [Actual Sequence] column (not mandatory, mostly
%    for oligo, as the sequence provided is precise!)
%

% NOTE: ArrayExpress's gpr.proc files are processed meta information
%       removed file containing only data table, whereas the .gpr file is
%       normal gpr files.  
%
%       Furthermore, the 'BLOCK' in gpr is replaced with 'Block Column',
%       'Block Row' in gpr.proc.
%
%       gpr.proc: sente E-MEXP-2193 (in ExperimentRawDataAutoParsing_20110408212659.log)
%       gpr: mayz E-MEXP-2862, sente E-MEXP-2888)
%
%       ArrayExpress is not ready to use format specific parsers used to
%       parse raw data file !!
%

It seems that extract platform information from raw data file is still
batter then try to map probes between adf and raw data file

How to justify this ?

As it is very difficult to programmatically connect platform information in
ADF file with that is provided in each raw data file, and also because every
file provide probe annotation information, we opt to extract probe annotation
used in raw data file to recontruct the platform probe information to avoid
remap probe information in raw data file to the corresponding probe
information in ADF file.
%
The consistency across the annotation information of an platform extracted
from different raw data file are check, and the inconsistency is reported.
%




%
% Part III: raw data parsing
%
A processed raw data format that removes metadata from original raw data
file, and added MIAMI tags to the data column which can be used to
distinguish data columns from platform columns.







\textit{The discrepancies in reporting expression values are in
  severalfold.
%
0) Due to the existence of the single-channel and dual-channel
platforms, each of which produce different amount of data,
%
1) Similar to the platform, there lacks of a standard data reporting format to
report expression value of each sample.  The existence of the single and dual
channel platforms further complicates the issue.  
%
2) Although the file extension could provide clues for the possible format of a
data file, it is not always reliable.  The standard extension 'txt' is widely
used by data files of different format.  Additionally, in GEO, often the sample
data are directly incroporated into the SOFT file as a table, hence lack of
this information.
%
3) There is no standard about the types of expression value to be reported.
For single platform, it could be the intensity value per probe, or per probe
set as in Affymetrix case.  For dual channel, this could be the raw intensity
value, the background corrected intensity values, or the normalized intensity
value, even the log ratio calculated based on intensity data of both channels.
%
This lack of standard creates wide-spread inconsistency across experiments
about the type of the intensity values they reported. 
%
4) There is no consistency with data column names to mark the type of the
data.  Although a free text description is provided for each column, it
cannot be readily analyzed by computer program.  This adds additional
complications for computerized data retrieving.  }















\subparagraph{Affymetrix platform data and expression value extraction}

\todo{Probe set, probe,  move !! virtual platform !! explanation into here}


The existing single platform compendia are based on Affymetrix
platforms. The use of the proprietary file formats to specify platform
information (CDF file) and to report expression values (CEL file)
avoids widespread data representation discrepencies, and simplifies
data retrieval.


For experiments using Affymetrix platform, the processed probe set expression
values are reported in GEO.
%
However, obtained using different normalization algorithms, these processed
values introduce artificial inconsistency among the data of different
experiment origins.
%
Therefore, the probe level expression values stored in Cel file (a proprietary
data reporting file format used by Affymetrix) are prefered.
%
Only when the Cel file is missing, the processed probe set expression values is
taken as an alternative.
%
Note that the background measurements obtained from mismatch probes are
discarded, as it has been shown that they are not reliable \cite{...}.
%
This process to handle the data generated on Affymetrix platforms is fully
automated.
%
To fully automate the data extraction for Cel file, we integrated the Fusion
SDK java libaray into our system.
%
% http://www.affymetrix.com/estore/partners_programs/programs/developer/fusion/index.affx?terms=no


\paragraph{FIVE: Miscellaneous issues NOT covered in the text, details}

- [AP] The lack of a unique access id for each microarray chip of an exp (Exp metadata extraction) \\
- [AP] The lack ch1, ch2, specification (Exp metadata extraction) \\
- [AP] The platform information extracted from raw data files (AP rawdata handling) \\
- 







\paragraph{OBSOLETE: expression value parsing}

% dual-channel platform
%
The data generated on dual-channel platforms are the most complicated,
as there are several types of expression values can be reported at two
different levels.

1) what type of the data to extract, what types are their, why \\
2) how to recognise the type of the data content from column name \\
3) conversion to obtain the deirable data


At the lower level, the per channel expression values are reported separately.
%
The simplest form is the raw intensity value ($I_i$) and the background value
($BG_i$) of each probe.

Furthermore, there are two raw intensity values, the mean value and the median
one.
%
Additionaly, it can also be the background corrected intensity
value($I_{bgci}$) calculated by subtracting the background from the raw
intensity.
%
At the higher level, the log ratio ($M$) and the average intensity value ($A$)
are reported.  They are calculated from the background corrected intensity
values of both channels based on the following formula.
%
\begin{eqnarray}
M = I_{bgc2} - I_{bgc1} \\
A = \frac{I_{bgc1} + I_{bgc2}}{2}
\end{eqnarray}

When the data quality condition required is met, the data are parsed
automatically. Otherwise, manual inspection is required.
%
The quality requirement are platform dependent.  


%
% generic single channel platform 
%
For other single channel platforms, there exist no standard data reporting
format.  However, as there is only one channel, the possible data types are
limited to either intensity value or background value.
%
As these two types of data are also the prefered data types for data obtained
in individual channel of a dual-channel platform, the data column
identification procedure designed to handle dual-channel data is applied.
%
The only difference is that there is data for only one channel.  
%
When both intensity and the background values can be identified, the data are
parsed automatically.  Otherwise, manual inspection is required to either
identify them or confirm the missing of the data.
%
\todo{Count how many of this kind single channel platform?}








\textbf{General workflow}

Figure

experiment information retrieve -> data separation -> experiment information
extraction









\subsubsection{Annotation}

\begin{itemize}
\item manual curation
\item controlled vocabulary
\item hierarchy (classification)
\item ontology (functional connection)
\end{itemize}


In a next phase, the condition contrasts that will be represented in the
compendium are defined and annotated.
%
Based on their biological role in an experimental survey, hybridizations are
labelled `reference' or `test' on a per experiment-and-platform combination
basis and matched to produce a set of condition contrasts.
%
For a single channel experiment, one or more hybridizations are chosen as 
references for the remaining tests. 
%
For dual channel experiments, usually one of every two array hybridizations
serves as a reference to the other, as this inherently counters much probe spot
associated variation in the measurements.
%
There are exceptions however, such as when one of the hybridizations on an
array does not constitute an identifiable and unique biological condition for
which the transcriptome was assessed (e.g. a sample of genomic DNA or a pool of
different samples that cannot be considered as biological replicates).
%
These hybridizations are discarded and the experiment is further treated as if
it was a single channel experiment.
%
In this way we ensure that every contrast has a biologically interpretable
meaning: its associated log-ratios measure changes in expression in response to
quantifiable stimuli that are altered from reference to test.


Using a set of formal hierarchically structured condition properties
(representing for instance mutations, compounds in the growth medium,
treatments, and general growth conditions), we can then specify the annotation
of each condition contrast rigidly as a vector representing the differences for
these property values between the test and reference condition.
%
This representation enables a mathematical comparison and automatic
organization of contrasts based on the conditions that are surveyed, but it is
a labor intensive manual curation process where information often needs to be
retrieved from original publications, supplementary data and occasionally
directly from the authors.
%
The condition properties themselves are further structured in a condition
ontology tree.
%
This ontology employs the same classes as the Gene Ontology biological process
subtree terms \cite{Gene2010} and maps the condition properties used to
annotate the condition contrasts to one or more biological processes or
functionalities they most likely affect.



\paragraph{Experiment design}

- Thesis Zhao Hui Section 2.2.2.1
- kristof's explanation about contrast and sample designation


\textbf{From Oncomine3 paper \cite{Rhodes2007}} p2

Because microarray data are only as valuable as the sample information
accompanying them, our data collection team places special
emphasis on sample facts curation and standardization. In
many cases, this permits us to test hypotheses not explored
in original analyses and publications 

When possible, sample facts are translated to standard terms used
by the NCI Thesaurus [10], allowing us to provide definitions
for clinical terms.




\cite{Parkinson2009} ATLAS.1
Use of the EFO allows
tuning of the ontology based on analysis of user queries
and provision of annotation at an appropriate level of
granularity for the database content. 




\subsubsection{Data homogenization}

\begin{itemize}
\item consistent per experiment normalization across
\item log ratio calculation
\end{itemize}

The final part in the creation of a compendium is the homogenization of the 
expression data: several preprocessing procedures are conducted to render 
expression levels comparable between different experiments and platforms. 
%
Crucial steps in this preprocessing are array-specific and depend on both the 
technological platform that was used to perform the experiment, as well as on 
the reported units of expression and the type of normalizations that might have 
already been done. 
%
In general we adhere to the following principles: 
%
1) whenever possible, raw intensities are preferred as data source over 
normalized data provided by the public repository, 
%
2) no local background or mismatch probe correction procedures are performed to 
avoid an increase in intensity error variance for lower, less reliable 
intensity levels \cite{Ritchie2007,Engelen2006,Li2001}, 
%
3) non-linear normalization techniques are performed to account for global 
inter-hybridization differences (e.g. loess fit to remove dye-related 
discrepancies on dual channel arrays \cite{Yang2002}, quantile normalization 
for high-density oligonucleotide experiments \cite{Bolstad2003}) and 
%
4) log-ratios are created for single-channel data according to the condition 
contrast definitions and combined with the dual channel measurements.




\paragraph{Quality Assessment}

Thesis Zhao Hui Section 2.2.2.2




\subsection{COMMAND, a web based system for expression compendium creation and management}


- web browser based frontend (user friendly, guidiance) 
- ruby based scripts (facilitate development and adaption, automated tasks) 
- rigid database design (a balance between size and utility) 

apache server + mysql db

javascript, ruby, php, etc 


\textbf{Database design}

http://www.codeproject.com/Articles/359654/important-database-designing-rules-which-I-fo

a modular structure that is the balance between 
- normalize to reduce redundancy
- de-normalization to improve performance
- database views as a middelware (refer below)

"a dimension and fact design"

A \textbf{middleware layer} was designed to provide a high-
level data model and application program interface to
insulate the details of the database schema from user
applications. 


\textbf{system architecture}

The system architecture is modular (a figure). 

5 subsystems: data parsing (interface, integrated data from both
repositories), annotation (interface, ??), homogenization, compendium data management
(release, revision, etc), system management (user management,
compendium management (properties, new compendium, etc) ).

brief about extra modules: system namagement, compenidum data management 
(release \& revision for data update,  release process)


\cite{Petryszak2013} ATLAS2013 p4  'Atlas infrastructure development'




\section{Results and Discussion}

\textbf{We created a methodology and created COMMAND system}

\textbf{Choice of extracting raw intensity data}


Although widely accepted community guideline such as MIAME \cite{Brazma2001}
greatly improved the reproducibility of experiments and the understanding of
the data by promoting a more precise description of experimental procedure, it
fails to provide detailed guidelines that facilitate the computerized analysis
of the data due to the lack of specifications for rigid data reporting formats.


\textbf{Choice of contrast and log-ratio}

In case a multiple-chip platform is used, a direct data merge across
chips is possible only for log-ratios data instead of intensity data,
as log-ratio cancel out the batch effects, whereas the batch effects
contained in the intensity data cannot be readily removed because
little or none genes are in common across chips.


\textbf{Choice for data homogenization}





compare with M3D and genevestigator compendium

\textbf{Discussion: }

\textbf{Check Introduction - Log-ratios and condition contrasts by 
\textit{kristof}}

log-ratio, contrast specification, condition hierarchy and ontology



ArrayExpress a lot of changes in SDRF file and the raw datas without
notifications, hard for developer to track and code needs to be adapted,

although the information provided are now more consistent ...


\textbf{Result: }

Table of raw data collection

Table of data annotation





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Keep the following \cleardoublepage at the end of this file, 
% otherwise \includeonly includes empty pages.
\cleardoublepage


% vim: tw=70 nocindent expandtab foldmethod=marker foldmarker={{{}{,}{}}}
